# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""Pipeline for quantizing a Llama model trained on TinyStories."""

from pathlib import cwd, Path

from max.graph import ops, Dim, Graph, Symbol, TensorType, Type
from max.graph.checkpoint import save, TensorDict
from max.graph.quantization import Q4_0Encoding

from pipelines.llama2.run import compile_graph, generate_text, Config
from pipelines.nn import (
    Attention,
    Embedding,
    FeedForward,
    RMSNorm,
    Transformer,
    TransformerBlock,
)
from pipelines.weights.download import download_weights_to_cache
from pipelines.weights.llama2checkpoint import LlamaCFile
from pipelines.weights.loadable_model import LlamaHParams
from .quantize_tinystories import quantize_and_save_tinystories
from .load_tinystories import load_quantized_tinystories


def quantize_tinystories_run():
    """Runs the quantize TinyStories pipeline."""
    cache_path = cwd().joinpath(".cache")
    checkpoint_path = cache_path.joinpath("stories15M_quantized.max")
    if not checkpoint_path.exists():
        quantize_and_save_tinystories(checkpoint_path)
    else:
        load_quantized_tinystories(checkpoint_path)
