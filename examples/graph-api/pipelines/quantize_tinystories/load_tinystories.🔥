# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""Pipeline for loading a quantized Llama model trained on TinyStories.

The code is almost identical to `quantize_tinystories.ðŸ”¥` except instead of
loading and quantizing weights from the karpathy/llama.c file, this loads the
already-quantized weights from the MAX checkpoint saved by the first run of the
pipeline.
"""

from collections import Optional
from pathlib import cwd, Path

from max.driver import cpu_device
from max.engine import InferenceSession
from max.graph import ops, Dim, Graph, Symbol, TensorType, Type
from max.graph.checkpoint import load, TensorDict
from max.graph.quantization import Float32Encoding, Q4_0Encoding

from max.graph import _OpaqueType as OpaqueType

from max.serve.kv_cache.types import (
    KVCacheLayout,
    KVCacheStaticParams,
    ContiguousKVCacheCollection,
)
from pipelines.llama2.run import compile_graph, generate_text, Config
from pipelines.llama3.metrics import Metrics
from pipelines.nn import (
    Embedding,
    FeedForward,
    RMSNorm,
    KVCacheOptimizedAttention,
    KVCacheOptimizedTransformer,
    KVCacheOptimizedTransformerBlock,
)
from pipelines.weights.download import download_to_cache
from pipelines.weights.loadable_model import LlamaHParams


@always_inline
def param_key(name: String, layer_idx: Optional[Int] = None) -> String:
    """Qualify parameter name with its layer index, if passed."""
    return name + "_" + str(layer_idx.value()) if layer_idx else name


def read_hyperparams_from_dict(
    tensor_dict: TensorDict,
) -> LlamaHParams:
    dims = tensor_dict.get[DType.int32]("hyperparams.dims")[0]
    n_layers = tensor_dict.get[DType.int32]("hyperparams.n_layers")[0]
    n_heads = tensor_dict.get[DType.int32]("hyperparams.n_heads")[0]
    norm_eps = tensor_dict.get[DType.float64]("hyperparams.norm_eps")[0]
    n_kv_heads = tensor_dict.get[DType.int32]("hyperparams.n_kv_heads")[0]
    vocab_size = tensor_dict.get[DType.int32]("hyperparams.vocab_size")[0]
    return LlamaHParams(
        dims=int(dims),
        n_layers=int(n_layers),
        n_heads=int(n_heads),
        norm_eps=norm_eps,
        n_kv_heads=int(n_kv_heads),
        vocab_size=int(vocab_size),
        head_dim=int(dims // n_heads),
        n_rep=int(n_heads // n_kv_heads),
    )


struct TeenyTinyLlama[kv_params: KVCacheStaticParams]:
    """Builder for a teeny tiny Llama 2 model trained on TinyStories."""

    alias batch_size = 1

    var hyperparams: LlamaHParams
    """Llama 2 hyperparameters, read from the checkpoint."""

    var quantized_params: TensorDict
    """Dictionary of quantized model parameters for checkpointing."""

    def __init__(inout self, model_path: Path):
        # Load quantized weights from MAX checkpoint.
        self.quantized_params = load(model_path)
        # Read Llama hyperparameters from the checkpoint.
        self.hyperparams = read_hyperparams_from_dict(self.quantized_params)

    def build(inout self) -> Graph:
        """Build the Llama 2 graph using the quantized weights from checkpoint.
        """
        # Set the KV cache and tokens input types.
        params = self.hyperparams
        cache_type = OpaqueType(
            ContiguousKVCacheCollection[DType.float32, kv_params].id()
        )
        tokens_type = TensorType(DType.int64, Self.batch_size, "seq_len")
        attn_mask_type = TensorType(DType.bool, self.batch_size, "full_seq_len")
        g = Graph(
            "TeenyTinyLlama",
            List[Type](tokens_type, attn_mask_type, cache_type),
        )

        @parameter
        def quantize(
            name: String, layer_idx: Optional[Int] = None
        ) -> (Symbol, String):
            """Stages a quantized parameter as a constant op in the graph."""
            # Load a parameter from the MAX checkpoint.
            param = self.quantized_params.get[DType.uint8](
                param_key(name, layer_idx)
            )

            # Stage a constant op in the graph and return it with its encoding.
            return g.constant(param), Q4_0Encoding.id()

        @parameter
        def norm_weight(
            name: String, layer_idx: Optional[Int] = None
        ) -> Symbol:
            """Stages a norm weight parameter as a constant op in the graph."""
            return g.constant(
                self.quantized_params.get[DType.float32](
                    param_key(name, layer_idx)
                )
            )

        def norm(name: String, layer: Optional[Int] = None) -> RMSNorm:
            w = norm_weight(name, layer)
            return RMSNorm(params.norm_eps, w)

        layers = List[KVCacheOptimizedTransformerBlock[kv_params]]()
        for layer in range(self.hyperparams.n_layers):
            # Stage a transformer block with quantized weights.
            # Read in float32 weights and quantize them before staging the op.
            # In the process, save references to the quantized tensors in the
            # parameter dictionary.
            # Doing so enables saving the checkpoint after building the graph,
            # complete with quantized weights.

            # KVCacheOptimizedAttention uses a fused qkv kernel
            # Setting wq/wk/wv to just use the Symbol, will compile
            # and return nans at the kernel level.
            # TODO: MSDK-842 is filed to build a workaround
            wq = quantize("attn_q", layer)[0]
            wk = quantize("attn_k", layer)[0]
            wv = quantize("attn_v", layer)[0]
            wqkv = ops.concat(List[Symbol](wq, wk, wv), axis=1)
            attention = KVCacheOptimizedAttention[kv_params](
                n_heads=params.n_heads,
                dim=params.dims,
                wqkv=wqkv.swapaxes(0, 1),
                wo=quantize("attn_output", layer),
            )

            feed_forward = FeedForward(
                w1=quantize("ffn_gate", layer),
                w2=quantize("ffn_down", layer),
                w3=quantize("ffn_up", layer),
            )

            layers.append(
                KVCacheOptimizedTransformerBlock[kv_params](
                    attention=attention,
                    feed_forward=feed_forward,
                    attention_norm=norm("attn_norm", layer),
                    ffn_norm=norm("ffn_norm", layer),
                )
            )

        # Stage the Llama 2 transformer model.
        embedding = Embedding(quantize("token_embd"))
        model = KVCacheOptimizedTransformer[kv_params](
            dim=params.dims,
            n_heads=params.n_heads,
            embedding=embedding,
            layers=layers,
            norm=norm("output_norm"),
            output=quantize("token_embd"),
            theta=10000.0,
        )

        outputs = model(tokens=g[0], mask=g[1], kv_cache=g[2])
        logits = outputs[0]
        g.output(List[Symbol](logits[-1, axis=1], outputs[1]))
        return g


def load_quantized_tinystories(checkpoint_path: Path):
    """Runs the TinyStories pipeline."""
    # Download and cache only the tokenizer config for Llama.
    tokenizer_path = download_to_cache(
        "https://github.com/tairov/llama2.mojo/raw/master/tokenizer.bin",
    )

    # Stage the Llama model graph from the saved MAX checkpoint file.
    # This is cpu only, and thus only a BHSD layout is needed
    alias layout = KVCacheLayout.BHSD
    alias kv_params = KVCacheStaticParams(
        num_heads=6, head_size=48, layout=layout
    )
    llama_builder = TeenyTinyLlama[kv_params](checkpoint_path)
    llama = llama_builder.build()

    # Generate text using the quantized Llama model and the provided prompt.
    metrics = Metrics()
    session = InferenceSession()

    config = Config()
    config.set("tokenizer-path", tokenizer_path)
    generate_text[layout](
        compile_graph(
            llama, cpu_device(), config.get("custom-ops-path")[List[Path]]
        ),
        llama_builder.hyperparams,
        config=config,
        metrics=metrics,
        execution_device=cpu_device(),
    )
    print()
    metrics.print()
