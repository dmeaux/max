# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""Pipeline for quantizing a Llama model trained on TinyStories."""

from collections import Optional
from pathlib import cwd, Path

from max.driver import cpu_device
from max.graph import Dim, Graph, Symbol, TensorType, Type, ops
from max.graph.checkpoint import save, TensorDict
from max.graph.quantization import (
    Float32Encoding,
    Q4_0Encoding,
    QuantizationEncoding,
)
from max.tensor import Tensor, TensorShape

from pipelines.llama2.tokenizer.bpe import BPETokenizer
from pipelines.llama2.run import (
    compile_graph,
    _generate_q_text_with_tokenizer,
    Config,
)
from pipelines.llama3.metrics import Metrics
from pipelines.nn import (
    Embedding,
    FeedForward,
    RMSNorm,
    Attention,
    TransformerBlock,
    Transformer,
)
from pipelines.weights.download import download_to_cache
from pipelines.weights.llama2checkpoint import LlamaCFile
from pipelines.weights.loadable_model import LlamaHParams


@always_inline
def param_key(name: String, layer_idx: Optional[Int] = None) -> String:
    """Qualify parameter name with its layer index, if passed."""
    return name + "_" + str(layer_idx.value()) if layer_idx else name


def add_hyperparams_to_dict(
    inout tensor_dict: TensorDict, hyperparams: LlamaHParams
):
    """Copies all hyperparameters into a TensorDict for later checkpointing."""
    tensor_dict.set(
        "hyperparams.dims",
        Tensor[DType.int32](TensorShape(1), hyperparams.dims),
    )
    tensor_dict.set(
        "hyperparams.n_layers",
        Tensor[DType.int32](TensorShape(1), hyperparams.n_layers),
    )
    tensor_dict.set(
        "hyperparams.n_heads",
        Tensor[DType.int32](TensorShape(1), hyperparams.n_heads),
    )
    tensor_dict.set(
        "hyperparams.vocab_size",
        Tensor[DType.int32](TensorShape(1), hyperparams.vocab_size),
    )
    tensor_dict.set(
        "hyperparams.norm_eps",
        Tensor[DType.float64](TensorShape(1), hyperparams.norm_eps),
    )
    tensor_dict.set(
        "hyperparams.n_kv_heads",
        Tensor[DType.int32](TensorShape(1), hyperparams.n_kv_heads),
    )


struct TeenyTinyLlama[
    encoding: QuantizationEncoding,
]:
    """Builder for a teeny tiny Llama 2 model trained on TinyStories."""

    alias batch_size = 1

    var params_file: LlamaCFile
    """Checkpoint file containing float32 Llama 2 weights."""

    var hyperparams: LlamaHParams
    """Llama 2 hyperparameters, read from the checkpoint."""

    var quantized_params: TensorDict
    """Dictionary of quantized model parameters for checkpointing."""

    def __init__(inout self, model_path: Path):
        """Initializes float32 model parameters from `model_path`."""
        self.params_file = LlamaCFile(model_path)

        # Read Llama hyperparameters from the checkpoint.
        self.hyperparams = self.params_file.hyperparams()

        # Initialize an empty set of parameters to checkpoint post quantization.
        self.quantized_params = TensorDict()
        add_hyperparams_to_dict(self.quantized_params, self.hyperparams)

    def build(inout self) -> Graph:
        """Build the Llama 2 graph, quantizing its weights by construction."""
        # Set the KV cache and tokens input types.
        params = self.params_file.hyperparams()
        cache_type = TensorType(
            DType.float32,
            "prev_seq_len",
            params.n_layers,
            Self.batch_size,
            params.n_kv_heads,
            params.head_dim,
        )
        tokens_type = TensorType(DType.int64, Self.batch_size, "seq_len")
        attn_mask_type = TensorType(DType.bool, self.batch_size, "full_seq_len")
        g = Graph(
            "TeenyTinyLlama",
            List[Type](tokens_type, attn_mask_type, cache_type, cache_type),
        )

        @parameter
        def quantize(
            name: String, layer_idx: Optional[Int] = None
        ) -> (Symbol, String):
            """Quantizes the parameter called `name` in the Llama 2 checkpoint.

            This function does the following:

            1. Quantizes the float32 parameter using the Q4_0 quantization
               encoding.
            2. Saves the resulting quantized parameter to this builder's
               parameter dictionary `self.quantized_params`.
               This is used to write out the checkpoint after graph building.
            3. Stages the quantized parameter as a constant op in the graph.
            """
            # 1. Quantize the parameter.
            q4_0_param = Q4_0Encoding.quantize(
                self.params_file.get[DType.float32](name, layer_idx)
            )

            # 2. Save the result in the parameter dictionary.
            self.quantized_params.set(
                key=param_key(name, layer_idx), value=q4_0_param
            )

            # 3. Stage the quantized constant op.
            return g.constant(q4_0_param), Q4_0Encoding.id()

        @parameter
        def weight[
            weight_type: QuantizationEncoding = encoding
        ](name: String, layer_idx: Optional[Int] = None) -> Symbol:
            """Stages a float32 parameter as a constant op in the graph."""
            # Load a float32 parameter from the Llama 2 checkpoint.
            float32_param = self.params_file.get[DType.float32](name, layer_idx)
            self.quantized_params.set(
                key=param_key(name, layer_idx), value=float32_param
            )

            # Stage a float32 constant op in the graph.
            return g.constant(float32_param)

        def norm(name: String, layer: Optional[Int] = None) -> RMSNorm:
            w = weight[Float32Encoding](name, layer)
            return RMSNorm(params.norm_eps, w)

        # Stage all of the transformer blocks in the stories15M Llama.
        layers = List[TransformerBlock[DType.float32]]()
        for layer in range(self.hyperparams.n_layers):
            # Stage a transformer block with quantized weights.
            # Read in float32 weights and quantize them before staging the op.
            # In the process, save references to the quantized tensors in the
            # parameter dictionary.
            # Doing so enables saving the checkpoint after building the graph,
            # complete with quantized weights.

            # KVCacheOptimizedAttention uses a fused qkv kernel
            # Setting wq/wk/wv to just use the Symbol, will compile
            # and return nans at the kernel level.
            # TODO: MSDK-842 is filed to build a workaround
            attention = Attention(
                n_heads=params.n_heads,
                n_kv_heads=params.n_kv_heads,
                head_dim=params.head_dim,
                dim=params.dims,
                use_custom_attention=True,
                wq=quantize("attn_q", layer),
                wk=quantize("attn_k", layer),
                wv=quantize("attn_v", layer),
                wo=quantize("attn_output", layer),
            )

            feed_forward = FeedForward(
                w1=quantize("ffn_gate", layer),
                w2=quantize("ffn_down", layer),
                w3=quantize("ffn_up", layer),
            )

            layers.append(
                TransformerBlock(
                    attention=attention,
                    feed_forward=feed_forward,
                    attention_norm=norm("attn_norm", layer),
                    ffn_norm=norm("ffn_norm", layer),
                )
            )

        # Stage the Llama 2 transformer model.
        embedding = Embedding(quantize("token_embd"))
        model = Transformer(
            dim=params.dims,
            n_heads=params.n_heads,
            embedding=embedding,
            layers=layers,
            norm=norm("output_norm"),
            output=quantize("token_embd"),
            theta=10000.0,
        )

        outputs = model(
            tokens=g[0], attention_mask=g[1], k_cache=g[2], v_cache=g[3]
        )
        logits = outputs[0]
        g.output(List[Symbol](logits[-1, axis=1], outputs[1], outputs[2]))
        return g


def quantize_and_save_tinystories(checkpoint_path: Path):
    """Runs the quantize TinyStories pipeline."""
    # Download and cache the weights for Llama trained on TinyStories.
    tokenizer_path = download_to_cache(
        "https://github.com/tairov/llama2.mojo/raw/master/tokenizer.bin",
    )

    model_path = download_to_cache(
        "https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin"
    )

    # Stage the Llama model graph.
    model = TeenyTinyLlama[Q4_0Encoding](model_path)
    graph = model.build()

    save(
        model.quantized_params,
        checkpoint_path,
    )

    params = model.hyperparams

    # Generate text using the quantized Llama model and the provided prompt.
    metrics = Metrics()

    execution_device = cpu_device()

    config = Config()
    config.set("tokenizer-path", tokenizer_path)
    config.set("model-path", model_path)

    compiled_model = compile_graph(
        graph, execution_device, config.get("custom-ops-path")[List[Path]]
    )

    mojo_tokenizer = BPETokenizer.from_file(config.get("tokenizer-path")[Path])
    _generate_q_text_with_tokenizer[BPETokenizer](
        mojo_tokenizer,
        compiled_model,
        params,
        config=config,
        execution_device=execution_device,
        metrics=metrics,
    )

    print()
    metrics.print()
