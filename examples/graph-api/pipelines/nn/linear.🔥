# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

"""Module containing layers related to linear transformations."""

from max.graph import ops, Symbol, TensorType, Dim
from max.graph.quantization import (
    Float32Encoding,
    BFloat16Encoding,
    QuantizationEncoding,
    Q4_0Encoding,
    Q4_KEncoding,
    Q6_KEncoding,
)


def linear_op(lhs: Symbol, rhs: Symbol) -> Symbol:
    g = lhs.graph()
    lhs_type = lhs.tensor_type()
    rhs_type = rhs.tensor_type()
    if rhs_type.rank() != 2:
        raise "rhs must be a matrix"

    # Compute shapes.
    lhs_shape = ops.shape_of(lhs)
    rhs_shape = ops.shape_of(rhs)
    last_lhs_axis = lhs_type.rank() - 1
    reshape_shape = ops.stack(
        List(g.scalar(Int64(-1)), lhs_shape[last_lhs_axis])
    )
    final_shape = ops.concat(List(lhs_shape[:last_lhs_axis], rhs_shape[0:1]))

    # Compute dims for reshape and matmul result types.
    final_dims = List[Dim]()
    for i in range(lhs_type.rank() - 1):
        final_dims.append(lhs_type.dim(i))
    final_dims.append(rhs_type.dim(0))

    matmul_dims = List[Dim]()
    matmul_dims.append(Dim.dynamic())
    matmul_dims.append(lhs_type.dim(-1))

    # Reshape LHS to a matrix, which is expected by the custom op.
    lhs_matrix = ops.reshape(lhs, reshape_shape, matmul_dims)

    matmul_out = ops.custom["linear_op"](
        List[Symbol](lhs_matrix, rhs),
        TensorType(
            lhs_matrix.tensor_type().dtype, Dim.dynamic(), Dim.dynamic()
        ),
    )

    # Reshape matmul output to restore the original rank(lhs) - 1 dimensions.
    return ops.reshape(matmul_out, final_shape, final_dims)


@value
struct Linear:
    """A fully-connected layer without activations."""

    var w: Symbol
    var encoding_id: String

    def __init__(inout self, w: Symbol):
        self.w = w
        # Default to float32 dtype if not provided.
        self.encoding_id = Float32Encoding.id()

    def __init__(inout self, encoded_weight: Tuple[Symbol, String]):
        self.w, self.encoding_id = encoded_weight

    def __call__(self, x: Symbol) -> Symbol:
        if self.encoding_id == Q4_0Encoding.id():
            return ops.qmatmul[Q4_0Encoding](x, self.w)
        elif self.encoding_id == Q4_KEncoding.id():
            return ops.qmatmul[Q4_KEncoding](x, self.w)
        elif self.encoding_id == Q6_KEncoding.id():
            return ops.qmatmul[Q6_KEncoding](x, self.w)
        elif (
            self.encoding_id == Float32Encoding.id()
            or self.encoding_id == BFloat16Encoding.id()
        ):
            if self.w.tensor_type().dtype == DType.bfloat16:
                return linear_op(x, ops.transpose_matrix(self.w))
            else:
                return x @ self.w

        raise "unsupported quantization encoding in Linear: " + self.encoding_id

    def __rmatmul__(self, lhs: Symbol) -> Symbol:
        return self(lhs)
