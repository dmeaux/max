# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""The attention mechanism used within the model."""

from math import isqrt
from max.tensor import Tensor, TensorShape

from max.graph import Graph, ops, Dim, Symbol, TensorType
from max.graph.quantization import Float32Encoding, QuantizationEncoding

from pipelines.nn import Linear


def rope(x: Symbol, freqs_cis: Symbol) -> Symbol:
    """Applies rotary positional embeddings (RoPE) to `x`.

    Args:
        x: Activation tensor with shape (batch, seq_len, n_kv_heads, head_dim).
        freqs_cis: Positional frequencies tensor with shape
            (seq_len, head_dim // 2, 2).

    Returns:
        Input activation tensor with rotary positional embeddings applied and
        the same shape as `x`.
    """
    x_complex = ops.as_interleaved_complex(x)
    x_dims = x_complex.type().tensor().dims

    freqs_cis_bcast = ops.unsqueeze(ops.unsqueeze(freqs_cis, 1), 0)

    x_re = x_complex[0, axis= -1]
    x_im = x_complex[1, axis= -1]

    freqs_re = freqs_cis_bcast[0, axis= -1].rebind(1, x_dims[1], 1, x_dims[3])
    freqs_im = freqs_cis_bcast[1, axis= -1].rebind(1, x_dims[1], 1, x_dims[3])

    rope_re = (x_re * freqs_re) - (x_im * freqs_im)
    rope_im = (x_re * freqs_im) + (x_im * freqs_re)
    rope_complex = ops.as_complex(rope_re, rope_im)

    return ops.reshape_like(rope_complex, x)


def attention_mask(
    g: Graph, start_pos: Dim, seq_len: Dim, activation_dtype: DType
) -> Symbol:
    # Mask out current sequence elements [i, j] where j > i with an
    # upper-triangular matrix filled with -inf.
    mask_val = ops.cast(
        g.full(Scalar[DType.float32].MIN, seq_len, seq_len), activation_dtype
    )
    mask = ops.band_part(
        mask_val,
        g.scalar[DType.int64](-1),
        num_upper=g.scalar[DType.int64](0),
        # Invert the mask from lower to upper.
        exclude=True,
    )

    # Compute attention scores only for the new sequence.
    # Hence for a matrix of scores of size (seqlen, cache_len + seqlen),
    # the only masked entries are (i, j) for j > cache_len + i, since row i
    # corresponds to token cache_len + i.
    return ops.concat(
        List[Symbol](
            ops.cast(
                g.full[DType.float32](0, seq_len, start_pos),
                activation_dtype,
            ),
            mask,
        ),
        axis=1,
        out_dim=Dim("full_seq_len"),
    )


@value
struct Attention[model_dtype: DType = DType.float32]:
    var n_heads: Int
    var n_kv_heads: Int
    var head_dim: Int
    var dim: Int
    var use_custom_attention: Bool
    """Use a custom flash attention kernel if set."""

    var wq: Linear
    var wk: Linear
    var wv: Linear
    var wo: Linear

    def repeat_kv(self, kv: Symbol) -> Symbol:
        """Repeats key/value tensors to match the number of query heads."""
        batch_size = kv.shape()[0]
        kv = kv.reshape(batch_size, -1, self.n_kv_heads, 1, self.head_dim)
        kv = ops.tile(
            kv, List[Int64](1, 1, 1, self.n_heads // self.n_kv_heads, 1)
        )
        return kv.reshape(batch_size, -1, self.n_heads, self.head_dim)

    def flash_attention(
        self,
        xq: Symbol,
        xk: Symbol,
        xv: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> Symbol:
        """Op calling into a custom "split KV cache" flash attention kernel.

        The custom flash attention kernel takes the previous KV cache and
        current KV tensors separately.
        This avoids data movement of the KV cache due to a `concat` op outside
        the flash attention kernel.
        """
        g = xq.graph()
        prev_seq_len = k_cache.shape()[0]
        seq_len = xq.shape()[1]

        attn_mask = attention_mask(g, prev_seq_len, seq_len, model_dtype)

        # Broadcast the attention mask across all attention heads.
        attn_shape = attn_mask.shape()
        attn_mask_bcast = ops.unsqueeze(
            ops.unsqueeze(attn_mask, axis=0), axis=0
        ).broadcast_to(1, self.n_heads, attn_shape[0], attn_shape[1])

        # Transpose operands to their expected layouts:
        # q and v: BHSD
        # k: BHSD
        # k_cache: 1BHSD
        # v_cache: 1BHSD
        return ops.custom["with_mask_flash_attention_split_kv_cpu"](
            List(
                xq.swapaxes(1, 2),
                xk.swapaxes(1, 2),
                xv.swapaxes(1, 2),
                k_cache.swapaxes(0, 1).swapaxes(1, 2).swapaxes(2, 3),
                v_cache.swapaxes(0, 1).swapaxes(1, 2).swapaxes(2, 3),
                attn_mask_bcast,
                g.constant(
                    Tensor(TensorShape(), isqrt(Float32(self.head_dim)))
                ),
            ),
            TensorType(model_dtype, 1, self.n_heads, "seq_len", self.head_dim),
        )

    def naive_attention(
        self,
        xq: Symbol,
        xk: Symbol,
        xv: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> Symbol:
        head_dim = xq.graph().scalar(self.head_dim, model_dtype)
        prev_seq_len = k_cache.shape()[0]
        seq_len = xq.shape()[1]

        full_seq_len = Dim("full_seq_len")
        keys = ops.concat(
            List[Symbol](k_cache, xk.swapaxes(0, 1)), out_dim=full_seq_len
        ).swapaxes(0, 1)
        values = ops.concat(
            List[Symbol](v_cache, xv.swapaxes(0, 1)), out_dim=full_seq_len
        ).swapaxes(0, 1)

        # Tile keys and values if this is GQA, otherwise no-op.
        keys = self.repeat_kv(keys)
        values = self.repeat_kv(values)

        xq = xq.swapaxes(1, 2)
        keys = keys.swapaxes(1, 2)
        values = values.swapaxes(1, 2)

        scores = (xq @ keys.swapaxes(2, 3)) * ops.rsqrt(head_dim)
        scores = scores + attention_mask(
            xq.graph(), prev_seq_len, seq_len, model_dtype
        )
        output = ops.softmax(scores) @ values
        return output

    def attention(
        self,
        xq: Symbol,
        xk: Symbol,
        xv: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> Symbol:
        if self.use_custom_attention:
            return self.flash_attention(xq, xk, xv, k_cache, v_cache)
        else:
            return self.naive_attention(
                xq,
                xk,
                xv,
                # Squeeze because naive_attention expects rank = 4 {k,v}_cache.
                ops.squeeze(k_cache, axis=1),
                ops.squeeze(v_cache, axis=1),
            )

    def __call__(
        self,
        input: Symbol,
        freqs_cis: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> (Symbol, Symbol, Symbol):
        """Computes attention on the input, reusing the KV cache.

        Args:
            input: Activations with shape (batch, seq_len, dim).
            freqs_cis: Positional frequencies tensor with shape
                (seq_len, head_dim // 2, 2).
            k_cache: Previously computed keys with shape
                (prev_seq_len, 1, batch, n_kv_heads, head_dim).
            v_cache: Previously computed values with shape
                (prev_seq_len, 1, batch, n_kv_heads, head_dim).

        Returns the result of multi-headed self attention on the input.
        """
        batch = input.shape()[0]
        seq_len = input.shape()[1]

        xq = input @ self.wq
        xk = input @ self.wk
        xv = input @ self.wv

        xq = xq.reshape(batch, seq_len, self.n_heads, self.head_dim)
        xk = xk.reshape(batch, seq_len, self.n_kv_heads, self.head_dim)
        xv = xv.reshape(batch, seq_len, self.n_kv_heads, self.head_dim)

        # Apply RoPE positional embeddings.
        xq = rope(xq, freqs_cis)
        xk = rope(xk, freqs_cis)

        output = self.attention(xq, xk, xv, k_cache, v_cache)

        output = output.swapaxes(1, 2).reshape(batch, seq_len, -1)

        return output @ self.wo, xk, xv
