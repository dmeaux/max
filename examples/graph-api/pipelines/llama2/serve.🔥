# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

import sys
from collections import List, Optional
from pathlib import cwd, Path
from python.python import _get_global_python_itf, Python, CPython
from runtime.llcl import Runtime, TaskGroup
from sys.ffi import DLHandle
from utils.index import Index


from max.engine import InferenceSession, Model, TensorMap
from max.engine._utils import handle_from_config, call_dylib_func
from max.graph import Graph
from max.graph.quantization import (
    Float32Encoding,
    QuantizationEncoding,
    Q4_0Encoding,
)
from max.serve.http.runtime import PythonEntry
from max.serve.server import InferenceServer
from max.serve.service import (
    InferenceRequest,
    InferenceResponse,
    InferenceService,
)
from max.serve._serve_rt import (
    CServerAsync,
    InferenceRequestImpl,
    InferenceResponseImpl,
)

from tensor import Tensor, TensorShape, TensorSpec

from .kv_cache import KVCache
from .model.llama import Llama2
from .tokenizer.bpe import BPETokenizer
from ..tokenizer import AutoTokenizer, Tokenizer
from ..weights.gguf import GGUFFile
from ..weights.llama2checkpoint import LlamaCFile
from ..weights.loadable_model import LlamaHParams, LoadableModel

from .run import (
    Config,
    compile_graph,
    execute,
    run_outer,
    _generate_text_with_tokenizer,
)


struct Llama2InferenceService[
    ModelT: LoadableModel,
    EncodingT: QuantizationEncoding = Float32Encoding,
    TokenizerT: Tokenizer = AutoTokenizer,
](InferenceService):
    """Inference service for Llama2."""

    var _config: Config
    var _tokenizer: TokenizerT
    var _session: InferenceSession

    var _lib: DLHandle
    var _server_ptr: DTypePointer[DType.invalid]

    var _model: Llama2[ModelT, EncodingT]
    var _graph: Graph
    var _compiled_model: Model

    fn __init__(
        inout self,
        owned config: Config,
        owned server_ptr: DTypePointer[DType.invalid],  # TODO: Remove
        owned tokenizer: TokenizerT,
        owned session: InferenceSession,
    ) raises:
        self._config = config^
        self._server_ptr = server_ptr
        self._tokenizer = tokenizer^
        self._session = session^
        self._lib = handle_from_config("serving", ".serve_lib")

        print("Building model...")
        self._model = Llama2[ModelT, EncodingT](
            self._config.model_path,
            enable_custom_rope_kernel=self._config.enable_custom_rope_kernel,
        )
        self._graph = self._model.build_graph("llama_model")
        self._compiled_model = self._session.load(
            self._graph, custom_ops_paths=self._config.custom_ops_paths
        )

    fn __del__(owned self):
        _ = self._config^
        _ = self._tokenizer^
        _ = self._session^
        _ = self._model^
        _ = self._graph^
        _ = self._compiled_model^

    fn init(self, inout server: InferenceServer) raises:
        server._impl.init(self._compiled_model)

    fn infer[
        req_type: InferenceRequest, resp_type: InferenceResponse
    ](inout self, request: req_type, inout response: resp_type) raises -> None:
        var respOr = Variant[resp_type, Error](response^)
        var rt = Runtime()
        rt.run(self.async_infer(request, respOr))
        if respOr.isa[Error]():
            raise respOr.unsafe_take[Error]()
        else:
            response = respOr.unsafe_take[resp_type]()

    fn handle_openai[
        handle_type: fn (PythonEntry) capturing raises -> None,
        req_type: InferenceRequest,
    ](self, request: req_type) raises:
        var api_type = request.get_api_type()
        var payload_type = request.get_payload_type()
        if api_type == 1:
            # OpenAI
            if payload_type == 0:
                # gRPC
                raise Error(
                    "OpenAI API compatibility is only supported via HTTP."
                )
            else:
                # HTTP
                var entry = PythonEntry()
                call_dylib_func[NoneType](
                    self._lib,
                    "M_OpenAIInferenceRequest_fillEntry",
                    self._server_ptr,
                    request.get_ptr(),
                    UnsafePointer.address_of(entry),
                )
                var cpython = _get_global_python_itf().cpython()
                var state = cpython.PyGILState_Ensure()
                handle_type(entry)
                cpython.PyGILState_Release(state)

    async fn async_infer[
        req_type: InferenceRequest, resp_type: InferenceResponse
    ](
        inout self, request: req_type, inout response: Variant[resp_type, Error]
    ) -> None:
        @parameter
        def handle(entry: PythonEntry) -> None:
            messages = List[String]()
            body = PythonObject(entry.request)
            for node in body["messages"]:
                messages.append(str(node["content"]))

            prompt = self._tokenizer.encode(messages[0], bos=String("\n<s>\n"))
            tokens = Tensor[DType.int64](TensorShape(1, len(prompt)))
            for i in range(len(prompt)):
                tokens[Index(0, i)] = prompt[i]

            outputs = List(self._tokenizer.decode(prompt))
            kv_cache = KVCache(
                self._model.model.hyperparams(),
                self._config.max_tokens,
                self._config.batch_size,
            )

            # The first iteration caches the entire prompt and all subsequent
            # iterations generate one token.
            # Avoid overrunning the cache by setting the trip count accordingly.
            for _ in range(prompt.size, self._config.max_tokens + 1):
                tokens = execute(
                    self._session, self._compiled_model, tokens, kv_cache
                )
                outputs.append(self._tokenizer.decode(tokens[0, 0]))

            var response = PythonObject(entry.response)
            var choices = response["choices"]
            for output in outputs:
                var choice = Python.dict()
                choice["message"] = output[]
                choices.append(choice)

        try:
            self.handle_openai[handle, req_type](request)
        except e:
            response.set[Error](e)


def serve_inner[
    ModelT: LoadableModel,
    EncodingT: QuantizationEncoding,
    TokenizerT: Tokenizer = AutoTokenizer,
](owned config: Config, owned tokenizer: TokenizerT):
    session = InferenceSession()
    server = InferenceServer.create("0.0.0.0:8000", session)
    service = Llama2InferenceService[ModelT, EncodingT, TokenizerT](
        config^, server._impl._impl._ptr, tokenizer^, session
    )
    print("Listening on port 8000!")
    service.init(server)
    server.serve(service)
    _ = server^
    _ = service^


def serve[
    ModelT: LoadableModel,
    EncodingT: QuantizationEncoding = Float32Encoding,
](config: Config):
    if AutoTokenizer.is_available():
        try:
            serve_inner[ModelT, EncodingT](
                config, AutoTokenizer("meta-llama/Llama-2-7b-hf")
            )
        except:
            print(
                "Unable to initialize AutoTokenizer, using Mojo tokenizer"
                " instead."
            )
            # Fall back to the Mojo tokenizer if setting up the AutoTokenizer
            # fails, for example due to lack of authentication.
            serve_inner[ModelT, EncodingT, BPETokenizer](
                config, BPETokenizer.from_file(config.tokenizer_path)
            )
    else:
        print(
            "Hugging Face `transformers` not installed, using Mojo tokenizer"
            " instead."
        )
        # Fall back to the Mojo tokenizer if `transformers` is not installed.
        serve_inner[ModelT, EncodingT, BPETokenizer](
            config, BPETokenizer.from_file(config.tokenizer_path)
        )


def llama2_serve():
    run_outer[serve]()
