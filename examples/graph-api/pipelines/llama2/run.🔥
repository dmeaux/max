# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

from collections import Dict
import sys
from math import align_up
from memory import memcpy
from os import setenv
from pathlib import Path, cwd
from utils.index import Index

from max.engine import InferenceSession, Model, SessionOptions
from max.driver import (
    AnyTensor,
    AnyMojoValue,
    Device,
    Tensor,
    cpu_device,
)
from max.driver._cuda import cuda_device
from max.graph import Graph
from max.graph.quantization import (
    Float32Encoding,
    QuantizationEncoding,
    Q4_0Encoding,
    Q4_KEncoding,
    Q6_KEncoding,
)

from max.serve.kv_cache.types import (
    ContiguousKVCache,
    ContiguousKVCacheManager,
    ContiguousKVCacheCollection,
    KVCacheLayout,
    KVCacheStaticParams,
)

from ..llama3.metrics import Metrics
from .model import Llama2
from ..samplers.weighted_sampler import WeightedSampler
from .tokenizer.bpe import BPETokenizer
from ..configs.common import get_max_tokens_to_generate
from ..configs.llama import (
    get_llama2_model_url,
    LlamaConfigRegistry,
    get_llama_base_default_config,
)
from ..configs.registry import ConfigRegistryDict
from ..configs.parse_args import (
    OptionTypeEnum,
    OptionValue,
    parse_args,
    register_pipeline_configs,
)
from ..tokenizer import AutoTokenizer, Tokenizer
from ..weights.download import download_to_cache, modular_cache_dir
from ..weights.gguf import GGUFFile
from ..weights.llama2checkpoint import LlamaCFile
from ..weights.loadable_model import LlamaHParams, LoadableModel


@value
struct Config:
    """Configuration for token generation runtime options."""

    var config: Dict[String, OptionValue]

    def __init__(inout self):
        config_registry = LlamaConfigRegistry(ConfigRegistryDict())

        default_configs = get_llama_base_default_config()
        self.config = register_pipeline_configs(
            config_registry.registry,
            parse_args(),
            default_configs,
        )

        # Check for invalid config
        model_path = self.config["model-path"]
        quantization_encoding = self.config["quantization-encoding"]
        if (
            model_path[Path].suffix() != ".gguf"
            and not quantization_encoding[String]
        ):
            raise (
                "`--model-path` must be `.bin` or `.gguf` file. "
                "Alternatively provide a `--quantization-encoding`"
            )

    fn get(inout self, key: String) raises -> OptionValue:
        """Returns an option value for `key` in the underlying config.

        Args:
            key: Key for the underlying config option.

        Returns:
            An OptionValue.

        Raises:
            An error for invalid key.
        """
        return self.config[key]

    fn set(inout self, key: String, val: OptionValue):
        """Sets a new value for a given config key. This will overwrite the old
        value if the key is already present.

        Args:
            key: A string based key for the underlying config option.
            val: A new value for a key that already exist.
        """
        self.config[key] = val


def compile_graph(
    graph: Graph,
    execution_device: Device,
    custom_ops_paths: List[Path] = List[Path](),
) -> Model:
    """Compiles a staged graph using the graph compiler."""
    session = InferenceSession(SessionOptions(execution_device))
    print("Compiling...")
    return session.load(graph, custom_ops_paths=custom_ops_paths)


def _generate_text_with_tokenizer[
    tokenizer_type: Tokenizer,
    kv_params: KVCacheStaticParams,
](
    inout tokenizer: tokenizer_type,
    compiled_model: Model,
    params: LlamaHParams,
    config: Config,
    inout metrics: Metrics,
    execution_device: Device,
):
    host_device = cpu_device()
    metrics.begin_timing_prompt()

    # Encode prompt and left pad-to-multiple-of
    prompt = tokenizer.encode(
        config.get("prompt")[String], bos=String("\n<s>\n")
    )
    padded_size = align_up(prompt.size, config.get("pad-to-multiple-of")[Int])
    n_pad_tokens = padded_size - prompt.size

    metrics.set_tokens_in_prompt(padded_size)

    # Allocate input & attention mask tensors, then initialize them.
    # FIXME (MSDK-774): Padding logic should be handled by tokenizer instead.
    tokens = Tensor[DType.int64, rank=2]((1, padded_size), host_device)
    prompt_attn_mask = Tensor[DType.bool, rank=2]((1, padded_size), host_device)
    for i in range(padded_size):
        tokens[0, i] = 0 if i < n_pad_tokens else prompt[i - n_pad_tokens]
        prompt_attn_mask[0, i] = False if i < n_pad_tokens else True

    max_tokens = get_max_tokens_to_generate(
        padded_size,
        config.get("max-length")[Int],
        config.get("max-new-tokens")[Int],
    )

    # Initialize Sampler
    sampler = WeightedSampler(
        config.get("temperature")[Float64].cast[DType.float32](),
        config.get("min-p")[Float64].cast[DType.float32](),
    )

    print("Executing...")
    print(tokenizer.decode(prompt), end="")

    kv_manager = ContiguousKVCacheManager[DType.float32, kv_params,](
        config.get("batch-size")[Int],
        max_tokens,
        params.n_layers,
        execution_device,
        host_device,
    )

    kv_collection = kv_manager.claim(config.get("batch-size")[Int])

    # The first iteration caches the entire prompt and all subsequent
    # iterations generate one token.
    # Avoid overrunning the cache by setting the trip count accordingly.
    metrics.begin_timing_generation()
    for i in range(padded_size, max_tokens + 1):
        if i == padded_size:
            valid_lengths = List[Int](padded_size)
        else:
            valid_lengths = List[Int](1)

        # Update Mask
        mask = Tensor[DType.bool, rank=2]((1, i), host_device)
        memcpy(
            mask.unsafe_ptr(),
            prompt_attn_mask.unsafe_ptr(),
            prompt_attn_mask.spec()[1],
        )
        for j in range(prompt_attn_mask.spec()[1], i):
            mask[0, j] = True

        result = compiled_model.execute(
            tokens.move_to(execution_device),
            mask.move_to(execution_device),
            AnyMojoValue(kv_collection^),
        )

        logits = (
            result[0]
            .take()
            .to_device_tensor()
            .move_to(host_device)
            .to_tensor[DType.float32, 2]()
        )

        kv_collection = (
            result[1]
            .take()
            .to[ContiguousKVCacheCollection[DType.float32, kv_params]]()
        )

        kv_manager.step(valid_lengths, kv_collection)

        token = SIMD[DType.int64, 1](sampler.sample(logits).selected)
        tokens = Tensor[DType.int64, rank=2]((1, 1), host_device)
        tokens[0, 0] = token

        metrics.new_token()
        print(tokenizer.decode(token), end="")
    print()
    metrics.end_timing()

    _ = kv_manager^
    _ = sampler^


def generate_text[
    layout: KVCacheLayout
](
    compiled_model: Model,
    params: LlamaHParams,
    config: Config,
    execution_device: Device,
    inout metrics: Metrics,
):
    """Generated text by applying the compiled model to the provided prompt."""
    mojo_tokenizer = BPETokenizer.from_file(config.get("tokenizer-path")[Path])
    if params.n_kv_heads == 6 and params.head_dim == 48:
        _generate_text_with_tokenizer[
            BPETokenizer,
            KVCacheStaticParams(num_heads=6, head_size=48, layout=layout),
        ](
            mojo_tokenizer,
            compiled_model,
            params,
            config=config,
            execution_device=execution_device,
            metrics=metrics,
        )
    else:
        raise "Unsupported n_kv_head (" + str(
            params.n_kv_heads
        ) + ") and head_dim (" + str(params.head_dim) + ")"


def run[
    model_type: LoadableModel,
    encoding: QuantizationEncoding,
    target: StringLiteral,
](config: Config) -> None:
    if config.get("prompt")[String] == "I believe the meaning of life is":
        print("Using default prompt, provide an argument to change it:")
        print('    --prompt "Hello llama3"')

    alias layout = KVCacheLayout.BHSD if target == "cpu" else KVCacheLayout.BSHD
    execution_device = cpu_device() if target == "cpu" else cuda_device()

    print("Building model...")
    metrics = Metrics()
    metrics.begin_timing_startup()

    model_params = model_type(config.get("model-path")[Path])
    params = model_params.hyperparams()
    if params.n_kv_heads == 6 and params.head_dim == 48:
        alias kv_params = KVCacheStaticParams(
            num_heads=6, head_size=48, layout=layout
        )
        model = Llama2[model_type, kv_params, encoding](
            model_params^,
        )
    else:
        raise "Unsupported model parameters. Got n_kv_heads=" + str(
            params.n_kv_heads
        ) + ", head_dim=" + str(params.head_dim)

    params = model.hyperparams()
    graph = model.build_graph("llama2")

    compiled_model = compile_graph(
        graph,
        execution_device,
        config.get("custom-ops-path")[List[Path]],
    )
    metrics.end_timing_startup()

    generate_text[layout](
        compiled_model, params, config, execution_device, metrics
    )

    print()
    metrics.print()


def llama2_run():
    config = Config()
    # TODO: Fix Quantization
    # For now, hardcode to float32
    # encoding = config.get("quantization-encoding")[String]
    encoding = Float32Encoding.id()

    if not config.get("model-path")[Path]:
        model_path = download_to_cache(get_llama2_model_url(encoding))
        config.set("model-path", model_path)

    if not config.get("tokenizer-path")[Path]:
        tokenizer_path = download_to_cache(
            "https://github.com/tairov/llama2.mojo/raw/master/tokenizer.bin",
        )
        config.set("tokenizer-path", tokenizer_path)

    if config.get("experimental-use-gpu")[Bool]:
        if encoding == Float32Encoding.id():
            run[LlamaCFile, Float32Encoding, "cuda"](config)
        else:
            raise encoding + " not available with 'experimental-use-gpu' option."

    else:
        if encoding == Float32Encoding.id():
            run[LlamaCFile, Float32Encoding, "cpu"](config)
        elif encoding == Q4_0Encoding.id():
            run[GGUFFile, Q4_0Encoding, "cpu"](config)
        elif encoding == Q4_KEncoding.id():
            run[GGUFFile, Q4_KEncoding, "cpu"](config)
        elif encoding == Q6_KEncoding.id():
            run[GGUFFile, Q6_KEncoding, "cpu"](config)
        else:
            raise "--quantization-encoding " + encoding + " not supported"
