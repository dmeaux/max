# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

import sys
from collections import List
from pathlib import cwd, Path
from utils.index import Index

from max.engine import InferenceSession, Model, TensorMap
from max.tensor import Tensor, TensorShape, TensorSpec

from .tokenizer.bpe import BPETokenizer
from .model.kv_cache import KVCacheView, cache_init, cache_update, cache_view
from .model.llama import Llama2
from ..weights.download import download_weights_to_cache
from ..weights.gguf import GGUFFile
from ..weights.llama2checkpoint import LlamaCFile
from ..weights.loadable_model import LoadableModel


@value
struct Config:
    """Configuration for token generation runtime options."""

    var batch_size: Int
    var float_dtype: String
    var model_name: String
    var model_path: Path
    var custom_ops_paths: List[Path]
    var tokenizer_path: Path
    var enable_custom_rope_kernel: Bool
    var prompt: String

    def __init__(
        inout self,
        /,
        batch_size: Int = 1,
        float_dtype: DType = DType.float32,
        model_name: String = "stories15M",
        model_path: Path = "",
        custom_ops_paths: List[Path] = List[Path](),
        tokenizer_path: Path = "",
        enable_custom_rope_kernel: Bool = False,
        prompt: String = "I believe the meaning of life is",
    ):
        self.batch_size = batch_size
        self.float_dtype = float_dtype
        self.model_name = model_name
        self.model_path = model_path
        self.custom_ops_paths = custom_ops_paths
        self.tokenizer_path = tokenizer_path
        self.enable_custom_rope_kernel = enable_custom_rope_kernel
        self.prompt = prompt

        self.parse_args()

    def parse_args(inout self):
        args = sys.argv()

        @parameter
        def read_value(index: Int) -> StringRef:
            if index >= len(args):
                raise "missing value for parameter `" + str(
                    args[index - 1]
                ) + "`"
            return args[index]

        # Skip the run_pipeline.mojo and llama2 arguments.
        i = 2
        while i < len(args):
            if args[i] == "--batch-size":
                self.batch_size = atol(read_value(i + 1))
                i += 2
            elif args[i] == "--float-dtype":
                self.float_dtype = read_value(i + 1)
                i += 2
            elif args[i] == "--model-name":
                name = read_value(i + 1)
                self.model_name = name
                i += 2
            elif args[i] == "--model-path":
                # If a manual model path has been specified, don't use one of
                # the downloadable pretrained models.
                self.model_name = ""
                self.model_path = Path(read_value(i + 1))
                i += 2
            elif args[i] == "--custom-ops-path":
                self.custom_ops_paths.append(Path(read_value(i + 1)))
                i += 2
            elif args[i] == "--tokenizer-path":
                self.tokenizer_path = Path(read_value(i + 1))
                i += 2
            elif args[i] == "--enable-custom-rope-kernel":
                self.enable_custom_rope_kernel = True
                i += 1
            elif args[i] == "--prompt":
                self.prompt = read_value(i + 1)
                i += 2
            else:
                raise "unsupported CLI argument: " + String(args[i])

        if self.enable_custom_rope_kernel and len(self.custom_ops_paths) == 0:
            raise "--custom-ops-path argument is required when --enable-custom-rope-kernel is set"


def execute[
    float_dtype: DType
](
    model: Model,
    session: InferenceSession,
    tokens: Tensor[DType.int64],
    k_cache_buff: KVCacheView[float_dtype],
    v_cache_buff: KVCacheView[float_dtype],
) -> TensorMap:
    constrained[
        float_dtype.is_floating_point(), "expected float inputs and outputs"
    ]()

    input_map = session.new_tensor_map()
    input_map.borrow("input0", tokens)
    input_map.borrow("input1", k_cache_buff.spec, k_cache_buff.ptr)
    input_map.borrow("input2", v_cache_buff.spec, v_cache_buff.ptr)
    result_map = model.execute(input_map)
    return result_map^


def run[
    ModelT: LoadableModel, float_dtype: DType = DType.float32
](config: Config):
    print("Initializing tokenizer...")
    var tokenizer = BPETokenizer.from_file(config.tokenizer_path)

    initial_prompt = "<s> " + config.prompt
    prompt = tokenizer.encode(initial_prompt, bos=String("\n<s>\n"))

    print("Building model...")
    var model = Llama2[ModelT, float_dtype](
        config.model_path,
        enable_custom_rope_kernel=config.enable_custom_rope_kernel,
    )
    module = model.build_graph("llama_model")
    session = InferenceSession()

    print("Compiling...")
    compiled_model = session.load(
        module, custom_ops_paths=config.custom_ops_paths
    )

    print("Executing...")
    max_tokens = 256
    print(initial_prompt, end="")

    k_cache_buff = cache_init[float_dtype](model, max_tokens, config.batch_size)
    v_cache_buff = cache_init[float_dtype](model, max_tokens, config.batch_size)

    tokens = Tensor[DType.int64](TensorShape(1, prompt.size))
    for i in range(prompt.size):
        # TODO(#29073): This should be `tokens[0, i] = prompt[i]`.
        tokens[Index(0, i)] = prompt[i].id

    cache_size = 0
    # The first iteration caches the entire prompt and all subsequent
    # iterations generate one token.
    # Avoid overrunning the cache by setting the trip count accordingly.
    for _ in range(prompt.size, max_tokens + 1):
        # Take a view of the backing KV cache buffer to avoid copying it.
        k_cache_view = cache_view(cache_size, k_cache_buff)
        v_cache_view = cache_view(cache_size, v_cache_buff)
        n_inputs = tokens.shape()[1]
        cache_size += n_inputs

        results = execute[float_dtype](
            compiled_model,
            session,
            tokens=tokens,
            k_cache_buff=k_cache_view,
            v_cache_buff=v_cache_view,
        )

        cache_update(results, "output1", k_cache_buff, k_cache_view, n_inputs)
        cache_update(results, "output2", v_cache_buff, v_cache_view, n_inputs)

        tokens = results.get[DType.int64]("output0")
        print(tokenizer.vocab[int(tokens[0, 0])].token, end="")

    print()


def llama2_run():
    config = Config()

    cache_path = cwd().joinpath(".cache")
    # If one of the downloadable models has been specified, download and cache
    # the weights and tokenizer for that model.
    if config.model_name == "stories15M":
        config.tokenizer_path = cache_path.joinpath("tokenizer.bin")
        config.model_path = cache_path.joinpath("stories15M.bin")
        download_weights_to_cache(
            cache_path,
            "https://github.com/tairov/llama2.mojo/raw/master/tokenizer.bin",
            "https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin",
        )
    elif config.model_name == "stories110M":
        download_weights_to_cache(
            cache_path,
            "https://github.com/tairov/llama2.mojo/raw/master/tokenizer.bin",
            "https://huggingface.co/karpathy/tinyllamas/resolve/main/stories110M.bin",
        )

    if config.model_path.suffix() == ".gguf":
        if config.float_dtype == str(DType.float16):
            run[GGUFFile, DType.float16](config)
        elif config.float_dtype == str(DType.float32):
            run[GGUFFile, DType.float32](config)
        else:
            raise "invalid float dtype"
    elif config.model_path.suffix() == ".bin":
        run[LlamaCFile](config)
    else:
        raise "invalid model path"
