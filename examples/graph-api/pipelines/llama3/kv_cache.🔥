# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""KV cache for the Transformer."""

from buffer import Buffer
from max._driver import AnyMemory, Device, DeviceTensor
from max.tensor import TensorSpec, TensorShape
from memory import memcpy
from max.tensor import Tensor, TensorSpec, TensorShape

from max.engine import EngineTensorView

from ..weights.loadable_model import LlamaHParams


@value
struct KVCache:
    """View into the KV cache backing `Tensor`."""

    var keys: DeviceTensor
    var values: DeviceTensor
    var sequence_length: Int

    def __init__(
        inout self,
        hp: LlamaHParams,
        max_length: Int,
        batch_size: Int,
        cpu_device: Device,
    ):
        spec = TensorSpec(
            DType.float32,
            max_length,
            hp.n_layers,
            batch_size,
            hp.n_kv_heads,
            hp.head_dim,
        )
        self.keys = cpu_device.allocate(spec, name=String("keys"))
        self.values = cpu_device.allocate(spec, name=String("values"))

        self.sequence_length = 0

    def update(inout self, owned keys: AnyMemory, owned values: AnyMemory):
        """Updates the KV Cache with data from new tokens."""
        cpu_device = self.keys.device()
        keys_tensor = keys.to_device_tensor().move_to(cpu_device)
        values_tensor = values.to_device_tensor().move_to(cpu_device)

        seqlen = keys_tensor.spec.bytecount() // self._offset(1)
        # This is doing the equivalent of
        #   self.keys[self.sequence_length:self.sequence_length + seqlen, ...] = keys
        #   self.values[self.sequence_length:self.sequence_length + seqlen, ...] = values
        start_pos = self._offset(self.sequence_length)
        update_size = self._offset(seqlen)
        memcpy(
            self.keys.unsafe_ptr() + start_pos,
            keys_tensor.unsafe_ptr(),
            update_size,
        )
        memcpy(
            self.values.unsafe_ptr() + start_pos,
            values_tensor.unsafe_ptr(),
            update_size,
        )
        _ = keys_tensor
        _ = values_tensor
        self.sequence_length += seqlen

    def keys_view(self, device: Device) -> DeviceTensor:
        """Copies the keys tensor from the cache for use in the model."""
        cpu_device = self.keys.device()
        keys_copy = DeviceTensor(
            self._spec(self.sequence_length), cpu_device, String("keys_view")
        )
        memcpy(
            keys_copy.unsafe_ptr(),
            self.keys.unsafe_ptr(),
            self._offset(self.sequence_length),
        )
        return keys_copy.move_to(device)

    def values_view(self, device: Device) -> DeviceTensor:
        """Copies the values tensor from the cache for use in the model."""
        cpu_device = self.values.device()
        values_copy = DeviceTensor(
            self._spec(self.sequence_length), cpu_device, String("values_view")
        )
        memcpy(
            values_copy.unsafe_ptr(),
            self.values.unsafe_ptr(),
            self._offset(self.sequence_length),
        )
        return values_copy.move_to(device)

    def _spec(self, seq_len: Int) -> TensorSpec:
        """The shape of a key or value tensor of a given sequence length."""
        s = self.keys.spec
        return TensorSpec(DType.float32, seq_len, s[1], s[2], s[3], s[4])

    def _offset(self, seq_len: Int) -> Int:
        """The memory size, in bytes, for a key or value tensor of a given sequence length.
        """
        return self._spec(seq_len).num_elements() * 4
