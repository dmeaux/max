# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

import sys
from collections import List, Optional
from pathlib import cwd, Path
from python.python import _get_global_python_itf, Python, CPython
from runtime.llcl import Runtime, TaskGroup
from sys.ffi import DLHandle
from utils.index import Index
from time import sleep

from max.engine import InferenceSession, Model, TensorMap
from max.engine._utils import handle_from_config, call_dylib_func
from max.graph import Graph
from max.graph.quantization import (
    Float32Encoding,
    QuantizationEncoding,
    Q4_0Encoding,
)
from max.serve.http.runtime import PythonEntry
from max.serve.server import InferenceServer
from max.serve.service import (
    InferenceRequest,
    InferenceResponse,
    InferenceService,
)
from max.serve._serve_rt import (
    CServerAsync,
    InferenceRequestImpl,
    InferenceResponseImpl,
)

from tensor import Tensor, TensorShape, TensorSpec

from .kv_cache import KVCache
from .model.llama import Llama3
from .tokenizer.bpe import BPETokenizer
from ..tokenizer import AutoTokenizer, Tokenizer
from ..weights.gguf import GGUFFile
from ..weights.loadable_model import LlamaHParams, LoadableModel

from .run import Config, compile_graph, execute, run_outer


struct Llama3InferenceService[
    ModelT: LoadableModel,
    EncodingT: QuantizationEncoding = Float32Encoding,
    TokenizerT: Tokenizer = AutoTokenizer,
](InferenceService):
    """Inference service for Llama3."""

    var _config: Config
    var _tokenizer: TokenizerT
    var _session: InferenceSession

    var _model: Llama3[ModelT, EncodingT]
    var _graph: Graph
    var _compiled_model: Model

    var _lib: DLHandle
    var _server_ptr: DTypePointer[DType.invalid]
    var _json_module: PythonObject

    fn __init__(
        inout self,
        owned config: Config,
        owned server_ptr: DTypePointer[DType.invalid],  # TODO: Remove
        owned tokenizer: TokenizerT,
        owned session: InferenceSession,
    ) raises:
        self._config = config^
        self._server_ptr = server_ptr
        self._tokenizer = tokenizer^
        self._session = session^
        self._lib = handle_from_config("serving", ".serve_lib")
        self._json_module = Python.import_module("json")

        print("Building model...")
        self._model = Llama3[ModelT, EncodingT](self._config.model_path)
        self._graph = self._model.build_graph("llama_model")
        self._compiled_model = self._session.load(
            self._graph, custom_ops_paths=self._config.custom_ops_paths
        )

    fn __del__(owned self):
        _ = self._config^
        _ = self._tokenizer^
        _ = self._session^
        _ = self._model^
        _ = self._graph^
        _ = self._compiled_model^

    fn init(self, inout server: InferenceServer) raises:
        server._impl.init(self._compiled_model)

    fn infer[
        req_type: InferenceRequest, resp_type: InferenceResponse
    ](inout self, request: req_type, inout response: resp_type) raises -> None:
        var respOr = Variant[resp_type, Error](response^)
        var rt = Runtime()
        rt.run(self.async_infer(request, respOr))
        if respOr.isa[Error]():
            raise respOr.unsafe_take[Error]()
        else:
            response = respOr.unsafe_take[resp_type]()

    fn handle_openai[
        handle_type: fn (PythonEntry) capturing raises -> None,
        req_type: InferenceRequest,
    ](self, request: req_type) raises:
        var api_type = request.get_api_type()
        var payload_type = request.get_payload_type()
        if api_type == 1:
            # OpenAI
            if payload_type == 0:
                # gRPC
                raise Error(
                    "OpenAI API compatibility is only supported via HTTP."
                )
            else:
                # HTTP
                var entry = PythonEntry()
                call_dylib_func[NoneType](
                    self._lib,
                    "M_OpenAIInferenceRequest_fillEntry",
                    self._server_ptr,
                    request.get_ptr(),
                    UnsafePointer.address_of(entry),
                )
                var cpython = _get_global_python_itf().cpython()
                var state = cpython.PyGILState_Ensure()
                handle_type(entry)
                cpython.PyGILState_Release(state)

    async fn async_infer[
        req_type: InferenceRequest, resp_type: InferenceResponse
    ](
        inout self, request: req_type, inout response: Variant[resp_type, Error]
    ) -> None:
        @parameter
        fn handle(entry: PythonEntry) raises -> None:
            var messages = List[String]()
            var body = PythonObject(entry.request)
            var stream = False
            if body.__contains__("stream") and body["stream"]:
                stream = True

            # Tokenize prompt and message contents.
            for node in body["messages"]:
                messages.append(str(node["content"]))
            var prompt = self._tokenizer.encode(
                messages[0], bos=String("\n<s>\n")
            )
            var tokens = Tensor[DType.int64](TensorShape(1, len(prompt)))
            for i in range(len(prompt)):
                tokens[Index(0, i)] = prompt[i]

            # Generate tokens until EOS or limit reached.
            var parent = PythonObject(entry.handler).parent
            var resp = PythonObject(entry.response)
            var outputs = List(self._tokenizer.decode(prompt))
            var kv_cache = KVCache(
                self._model.model.hyperparams(),
                self._config.max_tokens,
                self._config.batch_size,
            )
            if stream:
                parent.send_response(200)
                parent.send_header("Content-type", "text/event-stream")
                parent.end_headers()

            # The first iteration caches the entire prompt and all subsequent
            # iterations generate one token.
            # Avoid overrunning the cache by setting the trip count accordingly.
            for _ in range(prompt.size, self._config.max_tokens + 1):
                tokens = execute(
                    self._session, self._compiled_model, tokens, kv_cache
                )
                var next_token = self._tokenizer.decode(tokens[0, 0])
                if not stream:
                    outputs.append(next_token)
                else:
                    # Write chunk response if streaming.
                    var chunk = Python.dict()
                    var choices = Python.list()
                    var choice = Python.dict()
                    var delta = Python.dict()
                    delta["content"] = next_token
                    choice["delta"] = delta
                    choices.append(choice)
                    chunk["choices"] = choices

                    var json_str = self._json_module.dumps(chunk).encode(
                        encoding="utf_8"
                    )
                    try:
                        parent.wfile.write(json_str)
                        parent.wfile.flush()
                    except BrokenPipeError:
                        break

            # Write complete response if not streaming.
            if not stream:
                var choices = Python.list()
                for output in outputs:
                    var choice = Python.dict()
                    var message = Python.dict()
                    message["content"] = output[]
                    choice["message"] = message
                    choices.append(choice)
                resp["choices"] = choices

                parent.send_response(200)
                parent.send_header("Content-Type", "application/json")
                parent.end_headers()
                var json_str = self._json_module.dumps(resp).encode(
                    encoding="utf_8"
                )
                parent.wfile.write(json_str)

            # TODO: Add error handling.

        try:
            # TODO: Fold into the ProtocolHandler eventually.
            self.handle_openai[handle, req_type](request)
        except e:
            response.set[Error](e)


def serve_inner[
    ModelT: LoadableModel,
    EncodingT: QuantizationEncoding,
    TokenizerT: Tokenizer = AutoTokenizer,
](owned config: Config, owned tokenizer: TokenizerT):
    session = InferenceSession()
    server = InferenceServer.create[True]("0.0.0.0:8000", session)
    service = Llama3InferenceService[ModelT, EncodingT, TokenizerT](
        config^, server._impl._impl._ptr, tokenizer^, session
    )
    print("Listening on port 8000!")
    service.init(server)
    server.serve(service)
    _ = server^
    _ = service^


def serve[
    ModelT: LoadableModel,
    EncodingT: QuantizationEncoding = Float32Encoding,
](config: Config):
    tokenizer = AutoTokenizer("meta-llama/Meta-Llama-3-8B")
    serve_inner[ModelT, EncodingT](config, tokenizer^)


def llama3_serve():
    run_outer[serve]()
