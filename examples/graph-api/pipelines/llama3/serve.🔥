# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

import sys
from collections import List, Optional
from pathlib import cwd, Path
from python.python import _get_global_python_itf, Python, CPython
from runtime.llcl import Runtime, TaskGroup
from sys.ffi import DLHandle
from utils.index import Index
from time import sleep

from max.engine import InferenceSession, Model, SessionOptions, TensorMap
from max.engine._context import _Device
from max.engine._utils import handle_from_config, call_dylib_func
from max.graph import Graph
from max.graph.quantization import (
    Float32Encoding,
    QuantizationEncoding,
    Q4_0Encoding,
    Q4_KEncoding,
    Q6_KEncoding,
)
from max.serve.http import PythonServer, PythonService
from tensor import Tensor, TensorShape, TensorSpec

from .kv_cache import KVCache
from .model.llama import Llama3
from .tokenizer.tiktoken import TikTokenEncoder
from .tokenizer.bpe import TokenWithID
from .tokenizer.regex import set_locale_unicode
from ..samplers.weighted_sampler import WeightedSampler
from ..weights.gguf import GGUFArray, GGUFFile
from ..weights.loadable_model import LlamaHParams, LoadableModel

from .run import (
    Config,
    llama_main,
    cached_weights,
    compile_graph,
    execute,
)


struct Llama3InferenceService[
    EncodingT: QuantizationEncoding = Float32Encoding,
](PythonService):
    """Inference service for Llama3."""

    var _config: Config
    var _tokenizer: TikTokenEncoder
    var _session: InferenceSession

    var _model: Llama3[EncodingT]
    var _graph: Graph
    var _compiled_model: Model

    var _json_module: PythonObject

    fn __init__(
        inout self,
        owned config: Config,
        owned session: InferenceSession,
    ) raises:
        self._config = config^
        self._session = session^
        self._json_module = Python.import_module("json")

        print("Building model...")
        self._model = Llama3[EncodingT](self._config.model_path)
        self._graph = self._model.build_graph("llama_model")
        self._compiled_model = self._session.load(
            self._graph, custom_ops_paths=self._config.custom_ops_paths
        )
        self._tokenizer = TikTokenEncoder.cl100k_base_llama3(
            self._model.model["tokenizer.ggml.tokens"]._value.unsafe_get[
                GGUFArray
            ]()[]
        )

    fn __del__(owned self):
        _ = self._config^
        _ = self._tokenizer^
        _ = self._session^
        _ = self._model^
        _ = self._graph^
        _ = self._compiled_model^

    fn handle(
        inout self, owned body: PythonObject, owned handler: PythonObject
    ) raises -> None:
        if handler.path != "/v1/chat/completions":
            handler.send_response(500)
            handler.end_headers()
            return

        var stream = False
        if body.get("stream") is not None and body["stream"]:
            stream = True

        # Tokenize prompt and message contents.
        var prompt = List[TokenWithID](
            self._tokenizer.encode_special("<|begin_of_text|>")
        )

        for node in body["messages"]:
            prompt.append(self._tokenizer.encode_special("<|start_header_id|>"))
            prompt += self._tokenizer.encode(str(node["role"]), bos=None)
            prompt.append(self._tokenizer.encode_special("<|end_header_id|>"))
            prompt += self._tokenizer.encode(
                str("\n\n") + node["content"], bos=None
            )
            prompt.append(self._tokenizer.encode_special("<|eot_id|>"))
            # Only add a newline after "system" to follow the prompt format exactly at:
            #   https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/
            if str(node["role"]) != "system":
                prompt += self._tokenizer.encode("\n", bos=None)
        prompt.append(self._tokenizer.encode_special("<|start_header_id|>"))
        prompt += self._tokenizer.encode("assistant", bos=None)
        prompt.append(self._tokenizer.encode_special("<|end_header_id|>"))
        prompt += self._tokenizer.encode("\n", bos=None)

        var sampler = WeightedSampler(
            self._config.temperature, self._config.min_p
        )
        var tokens = Tensor[DType.int64](TensorShape(1, len(prompt)))
        for i in range(len(prompt)):
            tokens[Index(0, i)] = prompt[i].id

        var outputs = List[String]()
        var kv_cache = KVCache(
            self._model.model.hyperparams(),
            self._config.max_tokens,
            self._config.batch_size,
        )
        if stream:
            handler.send_response(200)
            handler.send_header("Content-type", "text/event-stream")
            handler.end_headers()

        # The first iteration caches the entire prompt and all subsequent
        # iterations generate one token.
        # Avoid overrunning the cache by setting the trip count accordingly.
        for _ in range(prompt.size, self._config.max_tokens + 1):
            var logits = execute(
                self._session, self._compiled_model, tokens, kv_cache
            )
            var token = sampler.sample(logits).selected
            tokens = Tensor(TensorShape(1, 1), Int64(token))

            # HACK: Check for end of text token.
            if tokens[Index(0, tokens.shape()[1] - 1)] == 128001:
                break

            # HACK: Check after decoding the token.
            var next_token = self._tokenizer.decode(token).token
            if next_token == "<|eot_id|>":
                break

            if not stream:
                outputs.append(next_token)
            else:
                # Write chunk response if streaming.
                var chunk = Python.dict()
                var choices = Python.list()
                var choice = Python.dict()
                var delta = Python.dict()
                delta["content"] = next_token
                choice["delta"] = delta
                choices.append(choice)
                chunk["choices"] = choices
                chunk["object"] = "chat.completion.chunk"

                var json_str = self._json_module.dumps(chunk).encode(
                    encoding="utf_8"
                )
                try:
                    handler.wfile.write(PythonObject("data: ").encode("utf-8"))
                    handler.wfile.write(json_str)
                    handler.wfile.write(PythonObject("\n\n").encode("utf-8"))
                    handler.wfile.flush()
                except BrokenPipeError:
                    break

        # Write complete response if not streaming.
        if stream:
            handler.wfile.write(PythonObject("data: [DONE]\n").encode("utf-8"))
            handler.wfile.flush()
        else:
            var raw_message: String = ""
            for output in outputs:
                raw_message += output[]

            var choice = Python.dict()
            var message = Python.dict()
            message["role"] = "assistant"
            message["content"] = raw_message
            choice["index"] = 0
            choice["message"] = message

            var choices = Python.list()
            choices.append(choice)
            var resp = Python.dict()
            resp["choices"] = choices
            var json_str = self._json_module.dumps(resp).encode(
                encoding="utf_8"
            )
            handler.wfile.write(json_str)


def serve[EncodingT: QuantizationEncoding](config: Config):
    session_options = SessionOptions(
        _device=_Device.CUDA if config.use_gpu else _Device.CPU
    )
    session = InferenceSession(session_options)
    service = Llama3InferenceService[EncodingT](config, session)
    server = PythonServer.create("0.0.0.0:8000")
    print("Listening on port 8000!")
    server.serve(service)


@value
struct ServingWeightMetadata:
    var main: llama_main
    var url: String

    @staticmethod
    def _ALL() -> Dict[String, ServingWeightMetadata]:
        all_metadata = Dict[String, ServingWeightMetadata]()
        all_metadata[Q4_0Encoding.id()] = ServingWeightMetadata(
            serve[Q4_0Encoding],
            "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-GGUF/resolve/main/Meta-Llama-3-8B.Q4_0.gguf",
        )
        all_metadata[Q4_KEncoding.id()] = ServingWeightMetadata(
            serve[Q4_KEncoding],
            "https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf",
        )
        all_metadata[Q6_KEncoding.id()] = ServingWeightMetadata(
            serve[Q6_KEncoding],
            "https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q6_K.gguf",
        )
        return all_metadata^

    @staticmethod
    def from_encoding(encoding_id: String) -> Self:
        return Self._ALL()[encoding_id]


def llama3_serve():
    config = Config()
    set_locale_unicode()

    encoding = config.quantization_encoding.lower()
    weight_metadata = ServingWeightMetadata.from_encoding(encoding)
    if not str(config.model_path):
        # TODO(MOCO-794): assign directly
        var model_path: String = str(cached_weights(weight_metadata.url))
        config = Config()
        config.model_path = model_path
    weight_metadata.main(config)
