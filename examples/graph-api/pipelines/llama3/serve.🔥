# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

from python import Python, PythonObject
from utils.index import Index
from pathlib import Path
from math import align_up

from max.engine import InferenceSession, Model, SessionOptions
from max.driver import cpu_device, Tensor
from max._utils import handle_from_config, call_dylib_func
from max.graph import Graph
from max.graph.quantization import QuantizationEncoding
from max.serve.http import PythonServer, PythonService
from max.tensor import TensorShape, TensorSpec

from .kv_cache import KVCache
from .model.llama import Llama3
from ..tokenizer.tiktoken import TikTokenEncoder
from ..tokenizer.bpe import TokenWithID
from ..tokenizer.regex import set_locale_unicode
from ..samplers.weighted_sampler import WeightedSampler
from ..weights.gguf import GGUFArray, GGUFFile
from ..weights.loadable_model import LlamaHParams, LoadableModel
from ..weights.download import download_to_cache
from ..configs.llama import get_llama3_model_url, get_llama3_1_model_url

from max.graph.quantization import (
    QuantizationEncoding,
    BFloat16Encoding,
    Float32Encoding,
    Q4_0Encoding,
    Q4_KEncoding,
    Q6_KEncoding,
)

from .run import (
    Config,
    compile_graph,
    get_max_tokens_to_generate,
    compile_graph,
    _get_attention_mask,
)


struct Llama3InferenceService[
    EncodingT: QuantizationEncoding,
](PythonService):
    """Inference service for Llama3."""

    var _config: Config
    var _tokenizer: TikTokenEncoder
    var _session: InferenceSession

    var _model: Llama3[EncodingT]
    var _compiled_model: Model

    var _json_module: PythonObject

    fn __init__(
        inout self,
        owned config: Config,
    ) raises:
        self._config = config^
        self._json_module = Python.import_module("json")
        self._model = Llama3[EncodingT](self._config.get("model-path")[Path])

        print("Loading tokenizer...")
        self._tokenizer = TikTokenEncoder.cl100k_base_llama3(
            self._model.model["tokenizer.ggml.tokens"]._value.unsafe_get[
                GGUFArray
            ]()[]
        )

        print("Building model...")
        if self._config.get("version")[String] == "3.0":
            model_name = "llama3"
        else:
            model_name = "llama3_1"
        self._session = InferenceSession(SessionOptions(cpu_device()))

        self._compiled_model = compile_graph(
            self._session,
            self._model.build_graph(model_name),
            self._config.get("custom-ops-path")[List[Path]],
        )

    fn handle(
        inout self, owned body: PythonObject, owned handler: PythonObject
    ) raises -> None:
        if handler.path != "/v1/chat/completions":
            handler.send_response(500)
            handler.end_headers()
            return

        cpu_device = cpu_device()

        stream = False
        if body.get("stream") is not None and body["stream"]:
            stream = True

        # Tokenize prompt and message contents.
        prompt = List[Int64](
            self._tokenizer.encode_special("<|begin_of_text|>")
        )

        for node in body["messages"]:
            prompt.append(self._tokenizer.encode_special("<|start_header_id|>"))
            prompt += self._tokenizer.encode(str(node["role"]), bos=None)
            prompt.append(self._tokenizer.encode_special("<|end_header_id|>"))
            prompt += self._tokenizer.encode(
                str("\n\n") + str(node["content"]), bos=None
            )
            prompt.append(self._tokenizer.encode_special("<|eot_id|>"))
            # Only add a newline after "system" to follow the prompt format exactly at:
            #   https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/
            if str(node["role"]) != "system":
                prompt += self._tokenizer.encode(String("\n"), bos=None)
        prompt.append(self._tokenizer.encode_special("<|start_header_id|>"))
        prompt += self._tokenizer.encode(String("assistant"), bos=None)
        prompt.append(self._tokenizer.encode_special("<|end_header_id|>"))
        prompt += self._tokenizer.encode(String("\n"), bos=None)

        sampler = WeightedSampler(
            self._config.get("temperature")[Float64].cast[DType.float32](),
            self._config.get("min-p")[Float64].cast[DType.float32](),
        )

        padded_size = align_up(
            prompt.size, self._config.get("pad-to-multiple-of")[Int]
        )
        n_pad_tokens = padded_size - prompt.size

        tokens = Tensor[DType.int64, rank=2](
            TensorShape(1, padded_size), cpu_device
        )
        prompt_attn_mask = Tensor[DType.bool, rank=2](
            (1, padded_size), cpu_device
        )
        for i in range(padded_size):
            tokens[0, i] = 0 if i < n_pad_tokens else prompt[i - n_pad_tokens]
            prompt_attn_mask[0, i] = False if i < n_pad_tokens else True

        print("--Prompt Received--")
        for token in prompt:
            print(self._tokenizer.decode(token[]), end="")

        print("Executing...")

        kv_cache = KVCache(
            self._model.model.hyperparams(),
            self._config.get("max-length")[Int],
            self._config.get("batch-size")[Int],
            cpu_device,
        )

        max_tokens = get_max_tokens_to_generate(
            padded_size,
            self._config.get("max-length")[Int],
            self._config.get("max-new-tokens")[Int],
        )

        if stream:
            handler.send_response(200)
            handler.send_header("Content-type", "text/event-stream")
            handler.end_headers()

        # The first iteration caches the entire prompt and all subsequent
        # iterations generate one token.
        # Avoid overrunning the cache by setting the trip count accordingly.
        outputs = List[String]()

        for i in range(padded_size, max_tokens + 1):
            results = self._compiled_model.execute(
                tokens.to_device_tensor().move_to(cpu_device),
                _get_attention_mask(prompt_attn_mask, i, cpu_device)
                .to_device_tensor()
                .move_to(cpu_device),
                kv_cache.keys_view(cpu_device),
                kv_cache.values_view(cpu_device),
            )

            kv_cache.update(results[1].take(), results[2].take())

            logits = results[0].take().to_device_tensor()
            logits = logits.move_to(cpu_device)
            logits_tensor = logits.to_tensor[DType.float32, rank=2]()
            token = Int64(sampler.sample(logits_tensor^).selected)

            tokens = Tensor[DType.int64, rank=2]((1, 1), cpu_device)
            tokens[0, 0] = token

            # HACK: Check for end of text token.
            if token == 128001:
                break

            # HACK: Check after decoding the token.
            next_token = self._tokenizer.decode(Int64(token))
            if next_token == "<|eot_id|>":
                break

            if not stream:
                outputs.append(next_token)
            else:
                # Write chunk response if streaming.
                chunk = Python.dict()
                choices = Python.list()
                choice = Python.dict()
                delta = Python.dict()
                delta["content"] = next_token
                choice["delta"] = delta
                choices.append(choice)
                chunk["choices"] = choices
                chunk["object"] = "chat.completion.chunk"

                json_str = self._json_module.dumps(chunk).encode(
                    encoding="utf_8"
                )
                try:
                    handler.wfile.write(PythonObject("data: ").encode("utf-8"))
                    handler.wfile.write(json_str)
                    handler.wfile.write(PythonObject("\n\n").encode("utf-8"))
                    handler.wfile.flush()
                except BrokenPipeError:
                    break

        # Write complete response if not streaming.
        if stream:
            handler.wfile.write(PythonObject("data: [DONE]\n").encode("utf-8"))
            handler.wfile.flush()
        else:
            raw_message = String()
            for output in outputs:
                raw_message += output[]

            choice = Python.dict()
            message = Python.dict()
            message["role"] = "assistant"
            message["content"] = raw_message
            choice["index"] = 0
            choice["message"] = message

            choices = Python.list()
            choices.append(choice)
            resp = Python.dict()
            resp["choices"] = choices
            json_str = self._json_module.dumps(resp).encode(encoding="utf_8")
            handler.send_response(200)
            handler.send_header("Content-type", "text/json")
            handler.end_headers()

            handler.wfile.write(json_str)


def serve[EncodingT: QuantizationEncoding](config: Config) -> None:
    service = Llama3InferenceService[EncodingT](config)
    server = PythonServer.create("0.0.0.0:8000")
    print("Listening on port 8000!")
    server.serve(service)


def llama3_serve():
    set_locale_unicode()

    config = Config()
    encoding = config.get("quantization-encoding")[String]

    if not config.get("model-path")[Path]:
        if config.get("version")[String] == "3.0":
            model_path = download_to_cache(get_llama3_model_url(encoding))
        else:
            model_path = download_to_cache(get_llama3_1_model_url(encoding))
        config.set("model-path", model_path)

    if encoding == Q4_0Encoding.id():
        serve[Q4_0Encoding](config)
    elif encoding == Q4_KEncoding.id():
        serve[Q4_KEncoding](config)
    elif encoding == Q6_KEncoding.id():
        serve[Q6_KEncoding](config)
    elif encoding == BFloat16Encoding.id():
        serve[BFloat16Encoding](config)
    elif encoding == Float32Encoding.id():
        serve[Float32Encoding](config)
    else:
        raise "--quantization-encoding " + encoding + " not supported"
