# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

import sys
from pathlib import Path
from utils.index import Index
from testing import assert_true

from max.engine import InferenceSession, Model, SessionOptions, TensorMap
from max._driver import cpu_device, cuda_device
from max.graph import Graph
from max.graph.quantization import (
    Float32Encoding,
    BFloat16Encoding,
    QuantizationEncoding,
    Q4_0Encoding,
    Q4_KEncoding,
    Q6_KEncoding,
)
from max.tensor import Tensor, TensorShape, TensorSpec

from .kv_cache import KVCache
from .metrics import Metrics
from .model.llama import Llama3
from ..tokenizer.tiktoken import TikTokenEncoder
from ..tokenizer.regex import set_locale_unicode
from ..configs.common import get_max_tokens_to_generate
from ..configs.llama import (
    LlamaConfigRegistry,
    get_llama_base_default_config,
    get_llama3_model_url,
)
from ..configs.registry import ConfigRegistryDict
from ..configs.parse_args import (
    OptionTypeEnum,
    OptionValue,
    parse_args,
    register_pipeline_configs,
)
from ..samplers.weighted_sampler import WeightedSampler
from ..weights.download import download_to_cache
from ..weights.gguf import GGUFArray, GGUFFile
from ..weights.loadable_model import LlamaHParams, LoadableModel


@value
struct Config:
    """Configuration for token generation runtime options."""

    var config: Dict[String, OptionValue]

    def __init__(inout self):
        config_registry = LlamaConfigRegistry(ConfigRegistryDict())

        default_configs = get_llama_base_default_config()
        self.config = register_pipeline_configs(
            config_registry.registry,
            parse_args(),
            default_configs,
        )

        @parameter
        if sys.has_neon():
            encoding = self.config["quantization-encoding"]
            if encoding[String] == "bfloat16":
                raise "bfloat16 is not currently supported on ARM"

    fn get(inout self, key: String) raises -> OptionValue:
        """Returns an option value for `key` in the underlying config.

        Args:
            key: Key for the underlying config option.

        Returns:
            An OptionValue.

        Raises:
            An error for invalid key.
        """
        return self.config[key]

    fn set(inout self, key: String, val: OptionValue):
        """Sets a new value for a given config key. This will overwrite the old
        value if the key is already present.

        Args:
            key: A string based key for the underlying config option.
            val: A new value for a key that already exist.
        """
        self.config[key] = val


def execute(
    session: InferenceSession,
    model: Model,
    tokens: Tensor[DType.int64],
    inout kv_cache: KVCache,
) -> Tensor[DType.float32]:
    """Execute the model predicting one new token."""
    input_map = session.new_tensor_map()
    input_map.borrow("input0", tokens)
    input_map.borrow("input1", kv_cache.keys_view())
    input_map.borrow("input2", kv_cache.values_view())
    results = model.execute(input_map)
    kv_cache.update(
        results.buffer[DType.float32]("output1"),
        results.buffer[DType.float32]("output2"),
    )
    return results.get[DType.float32]("output0")


def compile_graph(
    session: InferenceSession,
    graph: Graph,
    custom_ops_paths: List[Path] = List[Path](),
) -> Model:
    """Compiles a staged graph using the graph compiler."""
    print("Compiling...")
    return session.load(graph, custom_ops_paths=custom_ops_paths)


def generate_text(
    tokenizer: TikTokenEncoder,
    compiled_model: Model,
    params: LlamaHParams,
    config: Config,
    inout metrics: Metrics,
):
    """Generates text by applying the compiled model to the provided prompt."""

    metrics.begin_timing_prompt()

    prompt = tokenizer.encode(config.get("prompt")[String])
    sampler = WeightedSampler(
        config.get("temperature")[Float64].cast[DType.float32](),
        config.get("min-p")[Float64].cast[DType.float32](),
    )
    metrics.set_tokens_in_prompt(prompt.size)

    tokens = Tensor[DType.int64](TensorShape(1, len(prompt)))
    for i in range(len(prompt)):
        tokens[Index(0, i)] = prompt[i]

    # If a pipeline warmup is needed, create a throwaway KV cache and generate
    # a single output token from the model to exercise the graph.
    if config.get("warmup-pipeline")[Bool]:
        print("Warming up pipeline...")
        metrics.begin_timing_warmup()
        warmup_kv_cache = KVCache(
            params, config.get("max-tokens")[Int], config.get("batch-size")[Int]
        )
        _ = execute(
            compiled_model._session, compiled_model, tokens, warmup_kv_cache
        )
        warmup_token = Tensor(TensorShape(1, 1), Int64(123))
        _ = execute(
            compiled_model._session,
            compiled_model,
            warmup_token,
            warmup_kv_cache,
        )
        metrics.end_timing_warmup()

    print("Executing...")
    for token in prompt:
        print(tokenizer.decode(token[]), end="")

    kv_cache = KVCache(
        params, config.get("max-length")[Int], config.get("batch-size")[Int]
    )

    # The first iteration caches the entire prompt and all subsequent
    # iterations generate one token.
    # Avoid overrunning the cache by setting the trip count accordingly.
    metrics.begin_timing_generation()
    max_tokens = get_max_tokens_to_generate(
        prompt.size,
        config.get("max-length")[Int],
        config.get("max-new-tokens")[Int],
    )
    for _ in range(prompt.size, max_tokens + 1):
        logits = execute(
            compiled_model._session, compiled_model, tokens, kv_cache
        )
        token = Int64(sampler.sample(logits).selected)
        tokens = Tensor(TensorShape(1, 1), token)
        metrics.new_token()
        print(tokenizer.decode(token), end="")
    print()
    metrics.end_timing()


def run[encoding: QuantizationEncoding](config: Config) -> None:
    if config.get("prompt")[String] == "I believe the meaning of life is":
        print("Using default prompt, provide an argument to change it:")
        print('    --prompt "Hello llama3"')
    metrics = Metrics()
    metrics.begin_timing_startup()
    model = Llama3[encoding](config.get("model-path")[Path])
    params = model.hyperparams()

    print("Loading tokenizer...")
    tokenizer = TikTokenEncoder.cl100k_base_llama3(
        model.model["tokenizer.ggml.tokens"]._value.unsafe_get[GGUFArray]()[]
    )

    print("Building model...")
    graph = model.build_graph("llama3")

    session_options = SessionOptions(
        cuda_device() if config.get("experimental-use-gpu")[
            Bool
        ] else cpu_device()
    )
    session = InferenceSession(session_options)

    compiled_model = compile_graph(
        session, graph, config.get("custom-ops-path")[List[Path]]
    )
    metrics.end_timing_startup()

    generate_text(tokenizer, compiled_model, params, config, metrics)
    print()
    metrics.print()


def llama3_run():
    set_locale_unicode()

    config = Config()
    encoding = config.get("quantization-encoding")[String]

    if not config.get("model-path")[Path]:
        model_path = download_to_cache(get_llama3_model_url(encoding))
        config.set("model-path", model_path)

    if encoding == Q4_0Encoding.id():
        run[Q4_0Encoding](config)
    elif encoding == Q4_KEncoding.id():
        run[Q4_KEncoding](config)
    elif encoding == Q6_KEncoding.id():
        run[Q6_KEncoding](config)
    elif encoding == BFloat16Encoding.id():
        run[BFloat16Encoding](config)
    elif encoding == Float32Encoding.id():
        run[Float32Encoding](config)
    else:
        raise "--quantization-encoding " + encoding + " not supported"
