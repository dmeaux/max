# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""Common neural network layers used within the model."""

from max.graph import ops, Dim, Graph, TensorType, Type, Symbol
from max.graph.quantization import Q4_0Encoding
from max.graph.type import StaticDim
from max.tensor import Tensor, TensorShape

from pipelines.weights.ggml_quants import BlockQ40


@value
struct Linear[type: DType = DType.float32]:
    """A fully-connected layer without activations."""

    var w: Symbol

    def __call__(self, x: Symbol) -> Symbol:
        @parameter
        if type == DType.uint8:
            return ops.qmatmul[Q4_0Encoding](x, self.w)

        return x @ self.w

    def __rmatmul__(self, lhs: Symbol) -> Symbol:
        return self(lhs)


@value
struct FeedForward[type: DType = DType.float32]:
    var w1: Linear[type]
    var w2: Linear[type]
    var w3: Linear[type]

    def __call__(self, input: Symbol) -> Symbol:
        return (ops.silu(input @ self.w1) * (input @ self.w3)) @ self.w2


@value
struct RMSNorm:
    var eps: Float64
    var weight: Symbol

    def __call__(self, input: Symbol) -> Symbol:
        scale = ops.rsqrt(ops.mean(input**2.0, axis=-1) + self.eps)
        # Since norm weights are float32, cast to input dtype to avoid
        # promoting the result to float32 when the input is float16.
        return input * scale * ops.cast(self.weight, input.tensor_type().dtype)


@value
struct Embedding[type: DType = DType.float32]:
    """Quantized embedding (can be in GGML Q4_0 packed format)."""

    var weights: Symbol
    """The weights (maybe quantized Q4_0)."""

    def __call__(borrowed self, indices: Symbol) -> Symbol:
        """Gathers and dequantize rows of the quantized embedding, as needed.

        Args:
            indices: Rows of embedding to return.

        Returns:
            [Dequantized, as needed] embedding rows corresponding to `indices`.
        """

        # If there is no quantization, then just gather and return early.
        @parameter
        if type == DType.float32:
            return ops.gather(self.weights, indices, axis=0)

        g = self.weights.graph()
        quantized_tokens = ops.gather(self.weights, indices, axis=0)
        tokens_type = quantized_tokens.tensor_type()

        # Compute the dequantized output dim as the number of quantized blocks
        # times the number of elements (quants) per block.
        quantized_dim = self.weights.tensor_type().dims[-1]
        num_blocks = quantized_dim.value[StaticDim].dim // sizeof[BlockQ40]()
        alias quants_per_block = BlockQ40.QK4_0
        out_dim = int(BlockQ40.QK4_0 * num_blocks)
        # All embeddings in Llama 2 are 4096-vectors, so check this.
        if out_dim != 4096:
            raise "unexpected quantized weights dim " + str(out_dim)

        # Compute shapes to reshape embeddings to matrix input expected by
        # `ggml_q4_0_dequantize`.
        final_dims = List[Dim]()
        for i in range(tokens_type.rank() - 1):
            final_dims.append(tokens_type.dim(i))
        final_dims.append(out_dim)

        tokens_shape = ops.shape_of(quantized_tokens)
        last_tokens_axis = tokens_type.rank() - 1
        reshape_shape = ops.stack(
            List(g.scalar(Int64(-1)), tokens_shape[last_tokens_axis])
        )

        dequantize_dims = List[Dim]()
        dequantize_dims.append(Dim.dynamic())
        dequantize_dims.append(tokens_type.dim(-1))

        final_shape = ops.concat(
            List(
                tokens_shape[:last_tokens_axis],
                g.constant[DType.int64](
                    Tensor[DType.int64](TensorShape(1), out_dim)
                ),
            )
        )

        # Reshape gathered token embeddings to a matrix expected by dequantize.
        reshaped_tokens = ops.reshape(
            quantized_tokens, reshape_shape, dequantize_dims
        )

        # Dequantize to floating point.
        dequantized_tokens = ops.custom["ggml_q4_0_dequantize"](
            reshaped_tokens,
            TensorType(DType.float32, Dim.dynamic(), Dim.dynamic()),
        )

        # Restore original `rank(quantized_tokens) - 1` gather dimensions.
        return ops.reshape(dequantized_tokens, final_shape, final_dims)
