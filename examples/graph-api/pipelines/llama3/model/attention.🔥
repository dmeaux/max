# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""The attention mechanism used within the model."""

from .layers import Linear
from max.graph import ops, Symbol


def rope(x: Symbol, freqs_cis: Symbol) -> Symbol:
    """Applies rotary positional embeddings (RoPE) to `x`.

    Args:
        x: Activation tensor with shape (batch, seq_len, n_kv_heads, head_dim).
        freqs_cis: Positional frequencies tensor with shape
            (seq_len, head_dim // 2, 2).

    Returns:
        Input activation tensor with rotary positional embeddings applied and
        the same shape as `x`.
    """
    x_complex = ops.as_interleaved_complex(x)
    x_dims = x_complex.type().tensor().dims

    freqs_cis_bcast = ops.unsqueeze(ops.unsqueeze(freqs_cis, 1), 0)

    x_re = x_complex[0, axis= -1]
    x_im = x_complex[1, axis= -1]

    freqs_re = freqs_cis_bcast[0, axis= -1].rebind(1, x_dims[1], 1, x_dims[3])
    freqs_im = freqs_cis_bcast[1, axis= -1].rebind(1, x_dims[1], 1, x_dims[3])

    rope_re = (x_re * freqs_re) - (x_im * freqs_im)
    rope_im = (x_re * freqs_im) + (x_im * freqs_re)
    rope_complex = ops.as_complex(rope_re, rope_im)

    return ops.reshape_like(rope_complex, x)


def attention_mask(start_pos: Symbol, seq_len: Symbol) -> Symbol:
    g = start_pos.graph()

    seq_len = seq_len.reshape()
    start_pos = start_pos.reshape()

    # Mask out current sequence elements [i, j] where j > i with an
    # upper-triangular matrix filled with -inf.
    mask_val = g.full(Scalar[DType.float32].MIN, List[Symbol](seq_len, seq_len))
    mask = ops.band_part(
        mask_val,
        g.scalar[DType.int64](-1),
        num_upper=g.scalar[DType.int64](0),
        # Invert the mask from lower to upper.
        exclude=True,
    )

    # Compute attention scores only for the new sequence.
    # Hence for a matrix of scores of size (seqlen, cache_len + seqlen),
    # the only masked entries are (i, j) for j > cache_len + i, since row i
    # corresponds to token cache_len + i.
    return ops.concat(
        List[Symbol](g.full[DType.float32](0, List(seq_len, start_pos)), mask),
        axis=1,
    )


@value
struct Attention[type: DType = DType.float32]:
    var n_heads: Int
    var n_kv_heads: Int
    var head_dim: Int
    var dim: Int

    var wq: Linear[type]
    var wk: Linear[type]
    var wv: Linear[type]
    var wo: Linear[type]

    def __call__(
        self,
        input: Symbol,
        start_pos: Symbol,
        freqs_cis: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> (Symbol, Symbol, Symbol):
        g = input.graph()
        input_shape = ops.shape_of(input)

        batch_size = input_shape[0]
        seq_len = input_shape[1]
        head_dim = g.scalar[DType.float32](self.head_dim)

        xq = input @ self.wq
        xk = input @ self.wk
        xv = input @ self.wv

        xq = xq.reshape(batch_size, seq_len, self.n_heads, self.head_dim)
        xk = xk.reshape(batch_size, seq_len, self.n_kv_heads, self.head_dim)
        xv = xv.reshape(batch_size, seq_len, self.n_kv_heads, self.head_dim)

        xq = rope(xq, freqs_cis)
        xk = rope(xk, freqs_cis)

        keys = ops.concat(List(k_cache.swapaxes(0, 1), xk), axis=1)
        values = ops.concat(List(v_cache.swapaxes(0, 1), xv), axis=1)

        xq = xq.swapaxes(1, 2)
        keys = keys.swapaxes(1, 2)
        values = values.swapaxes(1, 2)

        scores = (xq @ keys.swapaxes(2, 3)) * ops.rsqrt(head_dim)
        scores = scores + attention_mask(start_pos, seq_len)
        output = ops.softmax(scores) @ values
        output = output.swapaxes(1, 2).reshape(batch_size, seq_len, -1)
        return output @ self.wo, xk, xv
