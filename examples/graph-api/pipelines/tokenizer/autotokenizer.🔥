# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""A tokenizer that leverages the Hugging Face transformers library."""

from math import ceildiv
from python import Python
from random import randint
from tensor import Tensor, TensorShape
from utils import Index

from . import Tokenizer


struct AutoTokenizer(Tokenizer):
    """Wrapper around generic tokenizers loaded from the huggingface
    transformers library.

    WARN: There are some extra copies that happen in the naive form of this
    tokenizer, specifically when converting python list -> numpy tensor -> MAX Tensor.
    """

    var _transformers_module: PythonObject
    var _numpy_module: PythonObject
    var _tokenizer_handle: PythonObject
    var _py_builtins_handle: PythonObject
    var _prev_token: Optional[Int64]

    def __init__(inout self, hf_tokenizer_name: String):
        self._transformers_module = Python.import_module("transformers")
        self._numpy_module = Python.import_module("numpy")
        self._tokenizer_handle = (
            self._transformers_module.AutoTokenizer.from_pretrained(
                hf_tokenizer_name
            )
        )
        self._py_builtins_handle = Python.import_module("builtins")
        self._prev_token = None

    @staticmethod
    def is_available() -> Bool:
        """Returns True if AutoTokenizer is available and False otherwise."""
        try:
            Python.import_module("transformers")
        except:
            return False
        else:
            return True

    @always_inline
    @staticmethod
    def _numpy_data_pointer[
        type: DType
    ](numpy_array: PythonObject) -> DTypePointer[type]:
        return DTypePointer[type](
            __mlir_op.`pop.index_to_pointer`[
                _type = __mlir_type[`!kgen.pointer<scalar<`, type.value, `>>`]
            ](
                Scalar[DType.index](
                    numpy_array.__array_interface__["data"][0].__index__()
                ).value
            )
        )

    @always_inline
    @staticmethod
    def _memcpy_to_numpy(array: PythonObject, tokens: List[Int64]):
        dst = AutoTokenizer._numpy_data_pointer[DType.int64](array)
        memcpy(dst, tokens.unsafe_ptr(), len(tokens))

    @always_inline
    @staticmethod
    def _memcpy_from_numpy(array: PythonObject, tensor: Tensor):
        src = AutoTokenizer._numpy_data_pointer[tensor.type](array)
        dst = tensor._ptr
        length = tensor.num_elements()
        memcpy(dst, src, length)

    @staticmethod
    @always_inline
    def _numpy_to_tensor[type: DType](array: PythonObject) -> Tensor[type]:
        shape = List[Int]()
        array_shape = array.shape
        for dim in array_shape:
            shape.append(dim.__index__())
        out = Tensor[type](shape)
        AutoTokenizer._memcpy_from_numpy(array, out)
        return out^

    @always_inline
    def _list_of_string_to_py_list(
        self, string_list: List[String]
    ) -> PythonObject:
        input_string_py = self._py_builtins_handle.list()
        for i in range(len(string_list)):
            input_string_py.append(string_list[i])

        return input_string_py

    @always_inline
    def _shape_to_python_list(self, shape: TensorShape) -> PythonObject:
        python_list = self._py_builtins_handle.list()
        for i in range(shape.rank()):
            python_list.append(shape[i])
        return python_list^

    @always_inline
    def _get_np_dtype[type: DType](self) -> PythonObject:
        @parameter
        if type.is_float32():
            return self._numpy_module.float32
        elif type.is_int32():
            return self._numpy_module.int32
        elif type.is_int64():
            return self._numpy_module.int64
        elif type.is_uint8():
            return self._numpy_module.uint8

        raise Error("Unknown datatype")

    @always_inline
    def _tokens_to_numpy(self, tokens: List[Int64]) -> PythonObject:
        shape = self._shape_to_python_list(len(tokens))
        tokens_as_numpy = self._numpy_module.zeros(
            shape, self._get_np_dtype[DType.int64]()
        )
        self._memcpy_to_numpy(tokens_as_numpy, tokens)
        return tokens_as_numpy

    def is_end_of_text(self, val: Int64) -> Bool:
        return val == int(self._tokenizer_handle.eos_token_id)

    def encode(
        self,
        input_string: List[String],
        bos: Optional[String] = None,
        eos: Optional[String] = None,
    ) -> List[Int64]:
        input_string_py = self._list_of_string_to_py_list(input_string)
        tokenized_py = self._tokenizer_handle(input_string_py)
        token_ids = AutoTokenizer._numpy_to_tensor[DType.int64](
            self._numpy_module.array(tokenized_py["input_ids"])
        )
        result = List[Int64]()
        for i in range(token_ids.num_elements()):
            result.append(token_ids._to_buffer()[i])
        _ = token_ids^

        return result

    def decode(inout self, output_tokens: List[Int64]) -> String:
        """Decodes tokens using the autotokenizer and accounts for spaces."""
        if not output_tokens:
            return ""

        # Keep track of the previous token.
        # This is needed because common tokenizers such as the Llama 2
        # sentencepiece tokenizer have different output depending on
        # neighbouring text.
        prev_token = self._prev_token
        self._prev_token = output_tokens[-1]

        decoded = self._tokenizer_handle.decode(
            self._tokens_to_numpy(output_tokens)
        )
        if not prev_token:
            return decoded

        # Attempt to produce correct output in a streaming setting.
        # The sentencepiece BPE tokenizer removes prefix space by default, so
        # keep the previous token around to correctly decode spaces.
        # Otherwise, when streaming one token at a time, no spaces are decoded.
        #
        # Note that even this approach still breaks when there are multiple
        # spaces in a row, for which we need a more general prefix length.
        #
        # See for example:
        # https://github.com/huggingface/transformers/issues/22710).
        prefixed_tokens = List[Int64]()
        prefixed_tokens.append(prev_token.value()[])
        for token in output_tokens:
            prefixed_tokens.append(token[])

        prefix_decoded = String(
            self._tokenizer_handle.decode(
                self._tokens_to_numpy(prefixed_tokens)
            )
        )
        if len(prefix_decoded) == len(decoded):
            # Also handle the case where the previous token is itself a space.
            return decoded

        return prefix_decoded[1:]
