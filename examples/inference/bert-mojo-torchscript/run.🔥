#!/usr/bin/env mojo
# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

from max.engine import InputSpec, Model, InferenceSession, EngineTensorSpec
from pathlib import Path
from python import Python
from tensor import TensorSpec
from utils.index import Index
import sys


def execute(model: Model, text: String, transformers: PythonObject) -> String:
    # The model was compiled with a maximum seqlen, so read that out from the model output metadata
    var output_spec = model.get_model_output_metadata()[0]
    var max_seqlen = output_spec[1].value()[]

    var tokenizer = transformers.AutoTokenizer.from_pretrained(
        "bert-base-uncased"
    )

    var inputs = tokenizer(
        text=text,
        add_special_tokens=True,
        padding="max_length",
        truncation=True,
        max_length=max_seqlen,
        return_tensors="np",
    )

    var input_ids = inputs["input_ids"]
    var token_type_ids = inputs["token_type_ids"]
    var attention_mask = inputs["attention_mask"]

    var outputs = model.execute(
        "input_ids",
        input_ids,
        "token_type_ids",
        token_type_ids,
        "attention_mask",
        attention_mask,
    )

    var result = outputs.get[DType.float32]("result0")

    var mask_idx = -1
    for i in range(len(input_ids[0])):
        if input_ids[0][i] == tokenizer.mask_token_id:
            mask_idx = i

    var predicted_token_id = result.argmax()[mask_idx]
    return tokenizer.decode(
        predicted_token_id,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=True,
    )


def load_model(session: InferenceSession) -> Model:
    var batch = 1
    var seqlen = 128

    var input_ids_spec = TensorSpec(DType.int64, batch, seqlen)
    var token_type_ids_spec = TensorSpec(DType.int64, batch, seqlen)
    var attention_mask_spec = TensorSpec(DType.int64, batch, seqlen)
    var options = List[InputSpec]()

    options.append(input_ids_spec)
    options.append(attention_mask_spec)
    options.append(token_type_ids_spec)

    var model = session.load(
        "../../models/bert-mlm.torchscript", input_specs=options
    )

    return model


def read_input() -> String:
    USAGE = (
        'Usage: ./run.mojo <str> \n\t e.g., ./run.mojo "Paris is the [MASK] of'
        ' France"'
    )

    argv = sys.argv()
    if len(argv) != 2:
        raise Error("\nPlease enter a prompt." + "\n" + USAGE)

    return sys.argv()[1]


fn main() raises:
    # Import HF Transformers dependency (for the tokenizer)
    var transformers = Python.import_module("transformers")

    # Read user prompt, create an InferenceSession, and load the model
    var prompt = read_input()
    var session = InferenceSession()
    var model = load_model(session)

    # Run inference
    var result = execute(model, prompt, transformers)

    print("input text: ", prompt)
    print("filled mask: ", prompt.replace("[MASK]", result))
